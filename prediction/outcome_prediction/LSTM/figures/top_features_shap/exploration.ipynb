{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from prediction.outcome_prediction.LSTM.testing.shap_helper_functions import check_shap_version_compatibility\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Requirements:\n",
    "- TensorFlow 1.14\n",
    "- Python 3.7\n",
    "- Protobuf downgrade to 3.20: `pip install protobuf==3.20`\n",
    "- downgrade h5py to 2.10: `pip install h5py==2.10`\n",
    "- turn off masking in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shap values require very specific versions\n",
    "check_shap_version_compatibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the JS visualization code to the notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_path = '/Users/jk1/temp/opsum_prediction_output/LSTM_72h_testing/3M_mRS02/2023_01_02_1057/test_LSTM_sigmoid_all_balanced_0.2_2_True_RMSprop_3M mRS 0-2_16_3/sigmoid_all_balanced_0.2_2_True_RMSprop_3M mRS 0-2_16_3.hdf5'\n",
    "features_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/preprocessed_features_01012023_233050.csv'\n",
    "labels_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/preprocessed_outcomes_01012023_233050.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = '3M mRS 0-2'\n",
    "masking = True\n",
    "units = 16\n",
    "activation = 'sigmoid'\n",
    "dropout = 0.2\n",
    "layers = 2\n",
    "optimizer = 'RMSprop'\n",
    "seed = 42\n",
    "test_size = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "override_masking_value = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.outcome_prediction.data_loading.data_formatting import format_to_2d_table_with_time\n",
    "\n",
    "# load the dataset\n",
    "X, y = format_to_2d_table_with_time(feature_df_path=features_path, outcome_df_path=labels_path,\n",
    "                                    outcome=outcome)\n",
    "\n",
    "n_time_steps = X.relative_sample_date_hourly_cat.max() + 1\n",
    "n_channels = X.sample_label.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from prediction.outcome_prediction.data_loading.data_formatting import features_to_numpy, \\\n",
    "    link_patient_id_to_outcome, numpy_to_lookup_table\n",
    "\n",
    "# Reduce every patient to a single outcome (to avoid duplicates)\n",
    "all_pids_with_outcome = link_patient_id_to_outcome(y, outcome)\n",
    "pid_train, pid_test, y_pid_train, y_pid_test = train_test_split(all_pids_with_outcome.patient_id.tolist(),\n",
    "                                                                all_pids_with_outcome.outcome.tolist(),\n",
    "                                                                stratify=all_pids_with_outcome.outcome.tolist(),\n",
    "                                                                test_size=test_size,\n",
    "                                                                random_state=seed)\n",
    "\n",
    "test_X_df = X[X.patient_id.isin(pid_test)]\n",
    "test_y_df = y[y.patient_id.isin(pid_test)]\n",
    "train_X_df = X[X.patient_id.isin(pid_train)]\n",
    "train_y_df = y[y.patient_id.isin(pid_train)]\n",
    "\n",
    "train_X_np = features_to_numpy(train_X_df,\n",
    "                                 ['case_admission_id', 'relative_sample_date_hourly_cat', 'sample_label', 'value'])\n",
    "test_X_np = features_to_numpy(test_X_df,\n",
    "                              ['case_admission_id', 'relative_sample_date_hourly_cat', 'sample_label', 'value'])\n",
    "train_y_np = np.array([train_y_df[train_y_df.case_admission_id == cid].outcome.values[0] for cid in\n",
    "                         train_X_np[:, 0, 0, 0]]).astype('float32')\n",
    "test_y_np = np.array([test_y_df[test_y_df.case_admission_id == cid].outcome.values[0] for cid in\n",
    "                      test_X_np[:, 0, 0, 0]]).astype('float32')\n",
    "\n",
    "\n",
    "# create look-up table for case_admission_ids, sample_labels and relative_sample_date_hourly_cat\n",
    "test_features_lookup_table = numpy_to_lookup_table(test_X_np)\n",
    "train_features_lookup_table = numpy_to_lookup_table(train_X_np)\n",
    "\n",
    "# Remove the case_admission_id, sample_label, and time_step_label columns from the data\n",
    "test_X_np = test_X_np[:, :, :, -1].astype('float32')\n",
    "train_X_np = train_X_np[:, :, :, -1].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.utils.scoring import precision, recall, matthews\n",
    "from prediction.outcome_prediction.LSTM.LSTM import lstm_generator\n",
    "\n",
    "model = lstm_generator(x_time_shape=n_time_steps, x_channels_shape=n_channels, masking=override_masking_value, n_units=units,\n",
    "                           activation=activation, dropout=dropout, n_layers=layers)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "              metrics=['accuracy', precision, recall, matthews])\n",
    "\n",
    "model.load_weights(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the training data for deep explainer => can use fewer instances\n",
    "explainer = shap.DeepExplainer(model, train_X_np)\n",
    "# explain the testing instances (can use fewer instances)\n",
    "# explaining each prediction requires 2 * background dataset size runs\n",
    "shap_values = explainer.shap_values(test_X_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the shap values\n",
    "with open(os.path.join('/Users/jk1/Downloads', 'temp_shap_values.pkl'),\n",
    "          'wb') as handle:\n",
    "    pickle.dump(shap_values, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(test_features_lookup_table['sample_label'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature importance\n",
    "Find most important features by mean absolute SHAP value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_most_important_features_by_mean_abs_shap = np.abs(shap_values[0]).mean(axis=(0, 1)).argsort()[::-1][0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(features)[ten_most_important_features_by_mean_abs_shap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Plot sum of shap value per feature (mean feature value color coded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0].sum(axis=(1))[:, ten_most_important_features_by_mean_abs_shap], pd.DataFrame(data=test_X_np.mean(axis=(1)), columns = features)[np.array(features)[ten_most_important_features_by_mean_abs_shap]],max_display=13, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Recreate this plot in seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_shap_values = shap_values[0].sum(axis=(1))[:, ten_most_important_features_by_mean_abs_shap]\n",
    "selected_shap_values_df = pd.DataFrame(data=selected_shap_values, columns = np.array(features)[ten_most_important_features_by_mean_abs_shap])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Join data in a common dataframe with shap values and feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_shap_values_df = selected_shap_values_df.reset_index()\n",
    "selected_shap_values_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_shap_values_df = selected_shap_values_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='shap_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_values_df =  pd.DataFrame(data=test_X_np.mean(axis=(1)), columns = features)[np.array(features)[ten_most_important_features_by_mean_abs_shap]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_values_df = selected_feature_values_df.reset_index()\n",
    "selected_feature_values_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_values_df = selected_feature_values_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='feature_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_shap_values_df = pd.merge(selected_shap_values_df, selected_feature_values_df, on=['case_admission_id_idx', 'feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_shap_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create color palette for feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_colors_palette = sns.color_palette(['#f61067', '#049b9a', '#012D98', '#a76dfe'], n_colors=4)\n",
    "all_colors_palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_colors = sns.color_palette(['#f61067', '#012D98'], n_colors=2)\n",
    "base_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.utils.visualisation_helper_functions import hex_to_rgb_color, create_palette\n",
    "from colormath.color_objects import sRGBColor, HSVColor, LabColor, LCHuvColor, XYZColor, LCHabColor, LuvColor\n",
    "\n",
    "start_color = '#012D98'\n",
    "end_color = '#f61067'\n",
    "\n",
    "# start_color= '#049b9a'\n",
    "# end_color= '#012D98'\n",
    "\n",
    "number_of_colors = 50\n",
    "\n",
    "start_rgb = hex_to_rgb_color(start_color)\n",
    "end_rgb = hex_to_rgb_color(end_color)\n",
    "\n",
    "palette = create_palette(start_rgb, end_rgb, number_of_colors, LabColor, extrapolation_length=1)\n",
    "custom_cmap = sns.color_palette(palette, n_colors=number_of_colors, as_cmap=True)\n",
    "sns.color_palette(palette, n_colors=number_of_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 2\n",
    "marker_size = 5 / n_features\n",
    "\n",
    "# ax = sns.swarmplot(data=features_with_shap_values_df, x=\"shap_value\", y=\"feature\", s=marker_size, hue='feature_value', alpha=1, palette=palette)\n",
    "plt.legend([], [], frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=test_X_np.mean(axis=(1)), columns = features)[np.array(features)[ten_most_important_features_by_mean_abs_shap]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data=features_with_shap_values_df,\n",
    "                  row=\"feature\",  hue='feature_value',\n",
    "                  height=3, aspect=4, palette=palette)\n",
    "g.map(sns.swarmplot, 'shap_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notes:\n",
    "- Swarm plot with dodge=True could be interesting to show the distribution of feature values for each feature, but takes a lot of time to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Final function\n",
    "\n",
    "Reusing Original code from SHAP\n",
    "\n",
    "Preqrequisites: pd.Dataframe with shap values and feature values for each feature, along with indexes for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plot_shap_direction_label = True\n",
    "plot_legend = False\n",
    "\n",
    "\n",
    "plt.gcf().set_size_inches(10, 10)\n",
    "\n",
    "row_height = 0.4\n",
    "alpha = 0.8\n",
    "\n",
    "for pos, feature in enumerate(features_with_shap_values_df.feature.unique()):\n",
    "    shaps = features_with_shap_values_df[features_with_shap_values_df.feature.isin([feature])].shap_value.values\n",
    "    values = features_with_shap_values_df[features_with_shap_values_df.feature.isin([feature])].feature_value\n",
    "    plt.axhline(y=pos, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
    "\n",
    "    values = np.array(values, dtype=np.float64)  # make sure this can be numeric\n",
    "\n",
    "    N = len(shaps)\n",
    "    nbins = 100\n",
    "    quant = np.round(nbins * (shaps - np.min(shaps)) / (np.max(shaps) - np.min(shaps) + 1e-8))\n",
    "    inds = np.argsort(quant + np.random.randn(N) * 1e-6)\n",
    "    layer = 0\n",
    "    last_bin = -1\n",
    "    ys = np.zeros(N)\n",
    "    for ind in inds:\n",
    "        if quant[ind] != last_bin:\n",
    "            layer = 0\n",
    "        ys[ind] = np.ceil(layer / 2) * ((layer % 2) * 2 - 1)\n",
    "        layer += 1\n",
    "        last_bin = quant[ind]\n",
    "    ys *= 0.9 * (row_height / np.max(ys + 1))\n",
    "\n",
    "    # trim the color range, but prevent the color range from collapsing\n",
    "    vmin = np.nanpercentile(values, 5)\n",
    "    vmax = np.nanpercentile(values, 95)\n",
    "    if vmin == vmax:\n",
    "        vmin = np.nanpercentile(values, 1)\n",
    "        vmax = np.nanpercentile(values, 99)\n",
    "        if vmin == vmax:\n",
    "            vmin = np.min(values)\n",
    "            vmax = np.max(values)\n",
    "    if vmin > vmax: # fixes rare numerical precision issues\n",
    "        vmin = vmax\n",
    "\n",
    "    # plot the non-nan values colored by the trimmed feature value\n",
    "    cvals = values.astype(np.float64)\n",
    "    cvals_imp = cvals.copy()\n",
    "    cvals_imp[np.isnan(cvals)] = (vmin + vmax) / 2.0\n",
    "    cvals[cvals_imp > vmax] = vmax\n",
    "    cvals[cvals_imp < vmin] = vmin\n",
    "    plt.scatter(shaps, pos + ys,\n",
    "               cmap=ListedColormap(palette), vmin=vmin, vmax=vmax, s=16,\n",
    "               c=cvals, alpha=alpha, linewidth=0,\n",
    "               zorder=3, rasterized=len(shaps) > 500)\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "axis_color=\"#333333\"\n",
    "if plot_legend:\n",
    "    m = cm.ScalarMappable(cmap=ListedColormap(palette))\n",
    "    m.set_array([0, 1])\n",
    "    cb = plt.colorbar(m, ticks=[0, 1], aspect=500)\n",
    "    cb.set_ticklabels(['FEATURE_VALUE_LOW', 'FEATURE_VALUE_HIGH'])\n",
    "    cb.set_label('Feature vlaue', size=12, labelpad=0)\n",
    "    cb.ax.tick_params(labelsize=11, length=0)\n",
    "    cb.set_alpha(1)\n",
    "    cb.outline.set_visible(False)\n",
    "    bbox = cb.ax.get_window_extent().transformed(plt.gcf().dpi_scale_trans.inverted())\n",
    "    cb.ax.set_aspect((bbox.height - 0.9) * 20)\n",
    "\n",
    "plt.gca().xaxis.set_ticks_position('bottom')\n",
    "plt.gca().yaxis.set_ticks_position('none')\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['left'].set_visible(False)\n",
    "plt.gca().tick_params(color=axis_color, labelcolor=axis_color)\n",
    "\n",
    "yticklabels = features_with_shap_values_df.feature.unique()\n",
    "plt.yticks(range(len(features_with_shap_values_df.feature.unique())), yticklabels, fontsize=13)\n",
    "plt.gca().tick_params('y', length=20, width=0.5, which='major')\n",
    "plt.gca().tick_params('x', labelsize=11)\n",
    "plt.ylim(-1, len(features_with_shap_values_df.feature.unique()))\n",
    "plt.xlabel('SHAP Value \\n (impact on model output)', fontsize=13)\n",
    "plt.grid(color='white', axis='y')\n",
    "\n",
    "plt.xlim(-0.25, 0.15)\n",
    "\n",
    "# Plot additional explanation with the shap value X axis\n",
    "if plot_shap_direction_label:\n",
    "    x_ticks_coordinates = plt.xticks()[0]\n",
    "    x_ticks_labels = [item.get_text() for item in plt.xticks()[1]]\n",
    "    # let x tick label be the coordinate with 2 decimals\n",
    "    x_ticks_labels = [f'{x_ticks_coordinate:.2f}' for x_ticks_coordinate in x_ticks_coordinates]\n",
    "    x_ticks_labels[0] = f'Toward worse \\n outcome'\n",
    "    x_ticks_labels[-1] = f'Toward better \\n outcome'\n",
    "    plt.xticks(x_ticks_coordinates, x_ticks_labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
