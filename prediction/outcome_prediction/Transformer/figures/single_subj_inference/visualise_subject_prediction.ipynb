{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b0dd0f",
   "metadata": {},
   "source": [
    "# Interactive visualisation of the prediction for a single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5ffb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:42:08.782672Z",
     "start_time": "2023-05-01T20:42:08.777936Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "from prediction.utils.utils import smooth, filter_consecutive_numbers\n",
    "from prediction.utils.visualisation_helper_functions import reverse_normalisation_for_subj, LegendTitle\n",
    "from preprocessing.preprocessing_tools.normalisation.reverse_normalisation import reverse_normalisation\n",
    "from prediction.utils.shap_helper_functions import check_shap_version_compatibility\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prediction.outcome_prediction.data_loading.data_formatting import format_to_2d_table_with_time, \\\n",
    "    link_patient_id_to_outcome, features_to_numpy, numpy_to_lookup_table\n",
    "from prediction.utils.visualisation_helper_functions import density_jitter\n",
    "from matplotlib.legend_handler import HandlerTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e3f2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:13:35.272512Z",
     "start_time": "2023-05-01T20:13:35.268992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shap values require very specific versions\n",
    "check_shap_version_compatibility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a68c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:13:35.285619Z",
     "start_time": "2023-05-01T20:13:35.272750Z"
    }
   },
   "outputs": [],
   "source": [
    "features_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/preprocessed_features_01012023_233050.csv'\n",
    "labels_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/preprocessed_outcomes_01012023_233050.csv'\n",
    "normalisation_parameters_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/logs_01012023_233050/normalisation_parameters.csv'\n",
    "cat_encoding_path = os.path.join(os.path.dirname(features_path), f'logs_{os.path.basename(features_path).split(\".\")[0].split(\"_\")[-2]}_{os.path.basename(features_path).split(\".\")[0].split(\"_\")[-1]}/categorical_variable_encoding.csv')\n",
    "\n",
    "\n",
    "shap_over_time_path = '/Users/jk1/temp/opsum_prediction_output/transformer/3M_mrs02/transformer_20230402_184459_test_set_evaluation/explanability/transformer_explainer_shap_values_over_ts_3m_mrs02_captum_n1449_all_72_cv2.pkl'\n",
    "mrs02_predictions_over_time_path = '/Users/jk1/temp/opsum_prediction_output/transformer/3M_mrs02/transformer_20230402_184459_test_set_evaluation/predictions_over_timesteps_cv2.pkl'\n",
    "death_predictions_over_time_path = '/Users/jk1/temp/opsum_prediction_output/transformer/3M_Death/testing/predictions_over_timesteps_cv1.pkl'\n",
    "\n",
    "out_dir = '/Users/jk1/Downloads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41acb9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:13:35.299161Z",
     "start_time": "2023-05-01T20:13:35.284045Z"
    }
   },
   "outputs": [],
   "source": [
    "outcome = '3M mRS 0-2'\n",
    "test_size = 0.2\n",
    "seed = 42\n",
    "n_splits = 5\n",
    "n_time_steps = 72\n",
    "total_n_features = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301131ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:13:35.312078Z",
     "start_time": "2023-05-01T20:13:35.288302Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a5e1eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:13:36.023079Z",
     "start_time": "2023-05-01T20:13:35.292496Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the shap values\n",
    "with open(shap_over_time_path, 'rb') as handle:\n",
    "    original_shap_values = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ba061",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:13:36.135267Z",
     "start_time": "2023-05-01T20:13:36.030895Z"
    }
   },
   "outputs": [],
   "source": [
    "only_last_timestep = False\n",
    "if only_last_timestep:\n",
    "    # use predictions from last timestep (as it also produces output for other timesteps)\n",
    "    shap_values = [original_shap_values[-1]]\n",
    "else:\n",
    "    shap_values = [np.array([original_shap_values[i][:, -1, :] for i in range(len(original_shap_values))]).swapaxes(0, 1)]\n",
    "shap_values_over_time = np.array(shap_values[0]).swapaxes(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f5eed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:13:36.316961Z",
     "start_time": "2023-05-01T20:13:36.291465Z"
    }
   },
   "outputs": [],
   "source": [
    "normalisation_parameters_df = pd.read_csv(normalisation_parameters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6e65d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:13:39.321731Z",
     "start_time": "2023-05-01T20:13:37.091944Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(mrs02_predictions_over_time_path, 'rb') as handle:\n",
    "    predictions_over_time = pickle.load(handle)\n",
    "predictions_over_time = np.array([prediction.numpy() for prediction in predictions_over_time])\n",
    "\n",
    "with open(death_predictions_over_time_path, 'rb') as handle:\n",
    "    death_predictions_over_time = pickle.load(handle)\n",
    "\n",
    "death_predictions_over_time = np.array([prediction.numpy() for prediction in death_predictions_over_time])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2816e737",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1319de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:14:50.574057Z",
     "start_time": "2023-05-01T20:13:40.531864Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = format_to_2d_table_with_time(feature_df_path=features_path, outcome_df_path=labels_path,\n",
    "                                    outcome=outcome)\n",
    "\n",
    "# Reduce every patient to a single outcome (to avoid duplicates)\n",
    "all_pids_with_outcome = link_patient_id_to_outcome(y, outcome)\n",
    "pid_train, pid_test, y_pid_train, y_pid_test = train_test_split(all_pids_with_outcome.patient_id.tolist(),\n",
    "                                                                all_pids_with_outcome.outcome.tolist(),\n",
    "                                                                stratify=all_pids_with_outcome.outcome.tolist(),\n",
    "                                                                test_size=test_size,\n",
    "                                                                random_state=seed)\n",
    "# Preprocess overall train data\n",
    "train_X_df = X[X.patient_id.isin(pid_train)]\n",
    "train_y_df = y[y.patient_id.isin(pid_train)]\n",
    "train_X_np = features_to_numpy(train_X_df,\n",
    "                               ['case_admission_id', 'relative_sample_date_hourly_cat', 'sample_label', 'value'])\n",
    "train_y_np = np.array([train_y_df[train_y_df.case_admission_id == cid].outcome.values[0] for cid in\n",
    "                       train_X_np[:, 0, 0, 0]]).astype('float32')\n",
    "train_X_np = train_X_np[:, :, :, -1].astype('float32')\n",
    "\n",
    "\n",
    "# Preprocess overall test data\n",
    "test_X_df = X[X.patient_id.isin(pid_test)]\n",
    "test_y_df = y[y.patient_id.isin(pid_test)]\n",
    "test_X_np = features_to_numpy(test_X_df,\n",
    "                              ['case_admission_id', 'relative_sample_date_hourly_cat', 'sample_label', 'value'])\n",
    "test_y_np = np.array([test_y_df[test_y_df.case_admission_id == cid].outcome.values[0] for cid in\n",
    "                      test_X_np[:, 0, 0, 0]]).astype('float32')\n",
    "# create look-up table for case_admission_ids, sample_labels and relative_sample_date_hourly_cat\n",
    "test_features_lookup_table = numpy_to_lookup_table(test_X_np)\n",
    "# Remove the case_admission_id, sample_label, and time_step_label columns from the data\n",
    "test_X_np = test_X_np[:, :, :, -1].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54b783",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:15:23.557567Z",
     "start_time": "2023-05-01T20:14:50.582656Z"
    }
   },
   "outputs": [],
   "source": [
    "death_X, death_y = format_to_2d_table_with_time(feature_df_path=features_path, outcome_df_path=labels_path,\n",
    "                                    outcome=\"3M Death\")\n",
    "\n",
    "# Reduce every patient to a single outcome (to avoid duplicates)\n",
    "death_all_pids_with_outcome = link_patient_id_to_outcome(death_y, \"3M Death\")\n",
    "death_pid_train, death_pid_test, death_y_pid_train, death_y_pid_test = train_test_split(death_all_pids_with_outcome.patient_id.tolist(),\n",
    "                                                                death_all_pids_with_outcome.outcome.tolist(),\n",
    "                                                                stratify=death_all_pids_with_outcome.outcome.tolist(),\n",
    "                                                                test_size=test_size,\n",
    "                                                                random_state=seed)\n",
    "# Preprocess overall test data\n",
    "death_test_y_df = death_y[death_y.patient_id.isin(death_pid_test)]\n",
    "overlapping_death_test_y_df = death_test_y_df[death_test_y_df.patient_id.isin(pid_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad555dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:15:23.570676Z",
     "start_time": "2023-05-01T20:15:23.567359Z"
    }
   },
   "outputs": [],
   "source": [
    "death_y_df = death_y[(death_y.patient_id.isin(death_pid_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d4fe78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:15:23.597817Z",
     "start_time": "2023-05-01T20:15:23.579566Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test, y_test = test_X_np, test_y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d5d7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:15:23.598178Z",
     "start_time": "2023-05-01T20:15:23.588536Z"
    }
   },
   "outputs": [],
   "source": [
    "original_features = list(test_features_lookup_table['sample_label'])\n",
    "feature_names = np.array(original_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5eed17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:17:23.558760Z",
     "start_time": "2023-05-01T20:15:23.591941Z"
    }
   },
   "outputs": [],
   "source": [
    "non_normalised_train_X_df = reverse_normalisation(train_X_df, normalisation_parameters_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ec0ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:17:23.569348Z",
     "start_time": "2023-05-01T20:17:23.557979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find subject indices in test pop that also exist in death_test population\n",
    "subjs_in_both_test_sets = test_y_df.reset_index()[test_y_df.reset_index().case_admission_id.isin(death_y_df.reset_index().case_admission_id)].index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0ee540",
   "metadata": {},
   "source": [
    "## Create working data frame\n",
    "Join data in a common dataframe with shap values and feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9551ad7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9345af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:19:02.208780Z",
     "start_time": "2023-05-01T20:19:02.199585Z"
    }
   },
   "outputs": [],
   "source": [
    "reverse_categorical_encoding = True\n",
    "pool_hourly_split_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6bb3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:19:05.517488Z",
     "start_time": "2023-05-01T20:19:02.369683Z"
    }
   },
   "outputs": [],
   "source": [
    "shap_values_df = pd.DataFrame()\n",
    "for ts in range(n_time_steps):\n",
    "    ts_shap_values_df = pd.DataFrame(data=np.array(shap_values[0]).swapaxes(0, 1)[ts], columns = np.array(original_features))\n",
    "    ts_shap_values_df = ts_shap_values_df.reset_index()\n",
    "    ts_shap_values_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)\n",
    "    ts_shap_values_df = ts_shap_values_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='shap_value')\n",
    "    ts_shap_values_df['time_step'] = ts\n",
    "    shap_values_df = shap_values_df.append(ts_shap_values_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2de8e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:19:50.934434Z",
     "start_time": "2023-05-01T20:19:05.526245Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_values_df = pd.DataFrame()\n",
    "for subj_idx in range(test_X_np.shape[0]):\n",
    "    subj_feature_values_df = pd.DataFrame(data=test_X_np[subj_idx, :, :], columns = np.array(feature_names))\n",
    "    subj_feature_values_df = reverse_normalisation_for_subj(subj_feature_values_df, normalisation_parameters_df)\n",
    "    subj_feature_values_df = subj_feature_values_df.reset_index()\n",
    "    subj_feature_values_df.rename(columns={'index': 'time_step'}, inplace=True)\n",
    "    subj_feature_values_df['case_admission_id_idx'] = subj_idx\n",
    "    subj_feature_values_df = subj_feature_values_df.melt(id_vars=['case_admission_id_idx', 'time_step'],  var_name='feature', value_name='feature_value')\n",
    "    feature_values_df = feature_values_df.append(subj_feature_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9715e52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:21:31.224288Z",
     "start_time": "2023-05-01T20:19:50.948097Z"
    }
   },
   "outputs": [],
   "source": [
    "shap_aggregation_func = 'sum' # 'median' or 'sum' (but sum makes mokes more sense)\n",
    "\n",
    "if reverse_categorical_encoding:\n",
    "    cat_encoding_df = pd.read_csv(cat_encoding_path)\n",
    "    for i in range(len(cat_encoding_df)):\n",
    "        cat_basename = cat_encoding_df.sample_label[i].lower().replace(' ', '_')\n",
    "        cat_item_list = cat_encoding_df.other_categories[i].replace('[', '').replace(']', '').replace('\\'', '').split(', ')\n",
    "        cat_item_list = [cat_basename + '_' + item.replace(' ', '_').lower() for item in cat_item_list]\n",
    "        for cat_item_idx, cat_item in enumerate(cat_item_list):\n",
    "            #  retrieve the dominant category for this subject (0 being default category)\n",
    "            feature_values_df.loc[feature_values_df.feature == cat_item, 'feature_value'] *= cat_item_idx + 1\n",
    "            feature_values_df.loc[feature_values_df.feature == cat_item, 'feature'] = cat_encoding_df.sample_label[i]\n",
    "            feature_values_df = feature_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).sum().reset_index()\n",
    "\n",
    "            shap_values_df.loc[shap_values_df.feature == cat_item, 'feature'] = cat_encoding_df.sample_label[i]\n",
    "            # sum the shap and feature values for each subject\n",
    "            if shap_aggregation_func:\n",
    "                shap_values_df = shap_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).sum().reset_index()\n",
    "            else:\n",
    "                shap_values_df = shap_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).median().reset_index()\n",
    "\n",
    "    # give a numerical encoding to the categorical features\n",
    "    cat_to_numerical_encoding = {\n",
    "        'Prestroke disability (Rankin)': {0:0, 1:5, 2:4, 3:2, 4:1, 5:3},\n",
    "        'categorical_onset_to_admission_time': {0:1, 1:2, 2:3, 3:4, 4:0},\n",
    "        'categorical_IVT': {0:2, 1:3, 2:4, 3:1, 4:0},\n",
    "        'categorical_IAT': {0:1, 1:0, 2:3, 3:2}\n",
    "    }\n",
    "\n",
    "    for cat_feature, cat_encoding in cat_to_numerical_encoding.items():\n",
    "        feature_values_df.loc[feature_values_df.feature == cat_feature, 'feature_value'] = feature_values_df.loc[feature_values_df.feature == cat_feature, 'feature_value'].map(cat_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96704a5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:22:13.765982Z",
     "start_time": "2023-05-01T20:21:31.365952Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# For features that are downsampled to hourly values, pool the values (median, min, max)\n",
    "if pool_hourly_split_values:\n",
    "    hourly_split_features = ['NIHSS', 'systolic_blood_pressure', 'diastolic_blood_pressure', 'mean_blood_pressure', 'heart_rate', 'respiratory_rate', 'temperature', 'oxygen_saturation']\n",
    "    for feature in hourly_split_features:\n",
    "        shap_values_df.loc[shap_values_df.feature.str.contains(feature), 'feature'] = (feature[0].upper() + feature[1:]).replace('_', ' ')\n",
    "        if shap_aggregation_func == 'median':\n",
    "            shap_values_df = shap_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).median().reset_index()\n",
    "        elif shap_aggregation_func == 'sum':\n",
    "            shap_values_df = shap_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).sum().reset_index()\n",
    "\n",
    "        feature_values_df.loc[feature_values_df.feature.str.contains(feature), 'feature'] = (feature[0].upper() + feature[1:]).replace('_', ' ')\n",
    "        feature_values_df = feature_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).median().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9021f39a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:22:14.122673Z",
     "start_time": "2023-05-01T20:22:13.766560Z"
    }
   },
   "outputs": [],
   "source": [
    "# Replace feature names with their english names\n",
    "feature_to_english_name_correspondence_path = os.path.join(os.path.dirname(\n",
    "    os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))))),\n",
    "                                                           'preprocessing/preprocessing_tools/feature_name_to_english_name_correspondence.xlsx')\n",
    "feature_to_english_name_correspondence = pd.read_excel(feature_to_english_name_correspondence_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ce121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:22:35.143579Z",
     "start_time": "2023-05-01T20:22:14.122941Z"
    }
   },
   "outputs": [],
   "source": [
    "for feature in shap_values_df.feature.unique():\n",
    "    if feature in feature_to_english_name_correspondence.feature_name.values:\n",
    "        shap_values_df.loc[shap_values_df.feature == feature, 'feature'] = feature_to_english_name_correspondence[feature_to_english_name_correspondence.feature_name == feature].english_name.values[0]\n",
    "\n",
    "for feature in feature_values_df.feature.unique():\n",
    "    if feature in feature_to_english_name_correspondence.feature_name.values:\n",
    "        feature_values_df.loc[feature_values_df.feature == feature, 'feature'] = feature_to_english_name_correspondence[feature_to_english_name_correspondence.feature_name == feature].english_name.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cdd43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:25:57.548903Z",
     "start_time": "2023-05-01T20:22:35.156751Z"
    }
   },
   "outputs": [],
   "source": [
    "use_simplified_shap_values = True\n",
    "\n",
    "if use_simplified_shap_values:\n",
    "    shap_values_over_time = []\n",
    "    for ts in range(n_time_steps):\n",
    "        subj_values_over_time = []\n",
    "        for subj in range(len(test_X_np)):\n",
    "            subj_values_over_time.append(shap_values_df[(shap_values_df.case_admission_id_idx == subj) & (shap_values_df.time_step == ts)].shap_value.values)\n",
    "        shap_values_over_time.append(np.array(subj_values_over_time))\n",
    "    shap_values_over_time = np.array(shap_values_over_time)\n",
    "\n",
    "    feature_names = shap_values_df.feature.unique()\n",
    "    feature_names = feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d016e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:14:05.317137Z",
     "start_time": "2023-04-29T19:12:58.882476Z"
    }
   },
   "source": [
    "## Choose subject and load prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729e61f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:26:22.734541Z",
     "start_time": "2023-05-01T20:26:22.723388Z"
    }
   },
   "outputs": [],
   "source": [
    "subj = randint(0, len(test_X_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9a12e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:26:22.906649Z",
     "start_time": "2023-05-01T20:26:22.899963Z"
    }
   },
   "outputs": [],
   "source": [
    "subj = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6679cd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:26:23.115419Z",
     "start_time": "2023-05-01T20:26:23.098859Z"
    }
   },
   "outputs": [],
   "source": [
    "print(subj, f'Outcome for {outcome}:', y_test[subj])\n",
    "print('Predicted probability of mrs02: ', predictions_over_time[-1,subj])\n",
    "\n",
    "# find index of row in death_y_df where case_admission_id == test_y_df.iloc[subj].case_admission_id\n",
    "subj_found_in_death_df = death_y_df.reset_index()[(death_y_df.reset_index().case_admission_id == test_y_df.iloc[subj].case_admission_id)]\n",
    "if len(subj_found_in_death_df) > 0:\n",
    "    death_idx = subj_found_in_death_df.index[0]\n",
    "    print('Predicted probability of mrs3-5: ', 1 - predictions_over_time[-1,subj] - death_predictions_over_time[-1,death_idx])\n",
    "    print('Predicted probability of death: ', death_predictions_over_time[-1,death_idx], f'(Outcome: {death_y_df.reset_index().iloc[death_idx].outcome})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f73399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:26:23.966428Z",
     "start_time": "2023-05-01T20:26:23.947731Z"
    }
   },
   "outputs": [],
   "source": [
    "subj_pred_over_ts = predictions_over_time[:,subj]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b24730",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T21:40:20.993489Z",
     "start_time": "2023-04-10T21:40:20.990480Z"
    }
   },
   "source": [
    "## Plot overall subject prediction & explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3a9c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T18:56:42.657730Z",
     "start_time": "2023-05-01T18:56:42.654602Z"
    }
   },
   "outputs": [],
   "source": [
    "n_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b34bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T18:56:43.255755Z",
     "start_time": "2023-05-01T18:56:42.655040Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot a bar plot showing impact of most important features on the prediction across all n_time_steps\n",
    "# find index of 3 features with biggest positive shap impart\n",
    "selected_positive_features = np.squeeze(shap_values_over_time[-1])[subj].argsort()[-n_features:][::-1]\n",
    "\n",
    "# find index of 3 features with biggest negative shap impart\n",
    "selected_negative_features = np.squeeze(shap_values_over_time[-1])[subj].argsort()[:n_features][::-1]\n",
    "\n",
    "selected_features = np.concatenate((selected_positive_features, selected_negative_features))\n",
    "\n",
    "fig1 = plt.figure(figsize=(15,5))\n",
    "ax1 = fig1.add_subplot(121)\n",
    "ax = sns.barplot(y=np.array(feature_names)[selected_features], x=np.squeeze(shap_values_over_time[-1])[subj][selected_features], palette=\"RdBu_r\")\n",
    "ax.title.set_text(f'SHAP values for subj {subj} ')\n",
    "\n",
    "if reverse_categorical_encoding:\n",
    "    non_norm_subj_df = feature_values_df[feature_values_df.case_admission_id_idx == subj].drop(columns=['case_admission_id_idx']).pivot(index='time_step', columns='feature', values='feature_value')\n",
    "    median_norm_feature_df = non_norm_subj_df.iloc[-1][np.array(feature_names)[selected_features]]\n",
    "else:\n",
    "    non_norm_subj_df = reverse_normalisation_for_subj(pd.DataFrame(data=test_X_np[subj], columns = feature_names), normalisation_parameters_df)\n",
    "    median_norm_feature_df = non_norm_subj_df.iloc[-1][np.array(feature_names)[selected_features]]\n",
    "\n",
    "ax2 = fig1.add_subplot(122)\n",
    "font_size=12\n",
    "bbox=[0, 0, 1, 1]\n",
    "ax2.axis('off')\n",
    "cell_text = []\n",
    "for row in range(len(median_norm_feature_df)):\n",
    "    cell_text.append([median_norm_feature_df.iloc[row].astype(str)])\n",
    "mpl_table = ax2.table(cellText = cell_text, rowLabels = median_norm_feature_df.index, bbox=bbox, colLabels=['Normalised value'], cellLoc='center', colLoc='center', loc='center')\n",
    "mpl_table.auto_set_font_size(False)\n",
    "mpl_table.set_fontsize(font_size)\n",
    "\n",
    "fig1.set_tight_layout(True)\n",
    "# set figure title\n",
    "fig1.suptitle(f'Explanation of prediction for subj {subj} with a probability of good outcome of {subj_pred_over_ts[-1]:.2f}', fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2085f82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T18:56:43.256155Z",
     "start_time": "2023-05-01T18:56:43.255464Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig1.savefig(os.path.join(out_dir, 'final_prediction.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(15,5))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "\n",
    "# plot a bar plot showing impact of most important features on the prediction across all n_time_steps\n",
    "# find index of 3 features with biggest positive shap impart\n",
    "selected_positive_features = np.squeeze(shap_values_over_time[-1])[subj].argsort()[-n_features:][::-1]\n",
    "# find index of 3 features with biggest negative shap impart\n",
    "selected_negative_features = np.squeeze(shap_values_over_time[-1])[subj].argsort()[:n_features][::-1]\n",
    "selected_features = np.concatenate((selected_positive_features, selected_negative_features))\n",
    "\n",
    "ax = sns.barplot(y=np.array(feature_names)[selected_features], x=np.squeeze(shap_values_over_time[-1])[subj][selected_features], palette=\"RdBu_r\", ax=ax1)\n",
    "\n",
    "non_norm_subj_df = feature_values_df[feature_values_df.case_admission_id_idx == subj].drop(columns=['case_admission_id_idx']).pivot(index='time_step', columns='feature', values='feature_value')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d6c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T19:48:27.333305Z",
     "start_time": "2023-05-01T19:48:27.290540Z"
    }
   },
   "source": [
    "## Plot relevant features in relation to training population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d923bf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T18:56:49.314521Z",
     "start_time": "2023-05-01T18:56:43.256023Z"
    }
   },
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(15, 12))\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.suptitle(\"Selected features\", fontsize=18, y=0.99, x=0.52, horizontalalignment='center')\n",
    "\n",
    "# set number of columns (use 3 to demonstrate the change)\n",
    "ncols = 3\n",
    "# calculate number of rows\n",
    "nrows = len(selected_features) // ncols + (len(selected_features) % ncols > 0)\n",
    "\n",
    "# loop through the length of features and keep track of index\n",
    "for n, feature in enumerate(selected_features):\n",
    "    # add a new subplot iteratively using nrows and cols\n",
    "    ax = plt.subplot(nrows, ncols, n + 1)\n",
    "\n",
    "    temp_pop_df = non_normalised_train_X_df[non_normalised_train_X_df.sample_label == feature_names[feature % total_n_features]]\n",
    "    sns.histplot(temp_pop_df.value, ax=ax)\n",
    "    plt.scatter(median_norm_feature_df[feature_names[feature % total_n_features]], 0, marker='o', s=500)\n",
    "    if (n % ncols) == 1:\n",
    "        if n <= len(selected_features) / 2:\n",
    "            ax.set_title(r\"$\\bf{Positive\\ features}$\" +f'\\n\\n{feature_names[feature % total_n_features]}')\n",
    "        else:\n",
    "            ax.set_title(r\"$\\bf{Negative\\ features}$\" + f'\\n\\n{feature_names[feature % total_n_features]}')\n",
    "\n",
    "    else:\n",
    "        ax.set_title(feature_names[feature % total_n_features])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9740e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T18:56:49.356726Z",
     "start_time": "2023-05-01T18:56:49.312108Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig2.savefig(os.path.join(out_dir, 'features_histogram_comparison.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8848eef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T18:56:51.518220Z",
     "start_time": "2023-05-01T18:56:49.354821Z"
    }
   },
   "outputs": [],
   "source": [
    "fig2_5 = plt.figure(figsize=(15, 12))\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.suptitle(\"Selected features\", fontsize=18, y=0.99, x=0.52, horizontalalignment='center')\n",
    "\n",
    "overlay_population_scatter = True\n",
    "overlay_subject_scatter = False\n",
    "plot_legend = True\n",
    "shap_value_base = 1000\n",
    "label_font_size = 12\n",
    "\n",
    "# set number of columns (use 3 to demonstrate the change)\n",
    "ncols = n_features\n",
    "# calculate number of rows\n",
    "nrows = 2\n",
    "\n",
    "# loop through the length of features and keep track of index\n",
    "for n, feature in enumerate(selected_features):\n",
    "    # add a new subplot iteratively using nrows and cols\n",
    "    ax = plt.subplot(nrows, ncols, n + 1)\n",
    "\n",
    "    temp_pop_df = non_normalised_train_X_df[non_normalised_train_X_df.sample_label == feature_names[feature % total_n_features]].groupby('case_admission_id').median()\n",
    "    sns.violinplot(temp_pop_df.value, ax=ax, color=feature_color_dict[feature], alpha=0.1, inner=None)\n",
    "    plt.setp(ax.collections, alpha=.1)\n",
    "\n",
    "    if overlay_population_scatter:\n",
    "        ys = density_jitter(temp_pop_df.value.values, width=0.25, cluster_factor=1)\n",
    "        ax.scatter(0 + ys, temp_pop_df.value, alpha=0.005, color='grey')\n",
    "\n",
    "    if overlay_subject_scatter:\n",
    "        ys = density_jitter(non_norm_subj_df[np.array(feature_names)[feature % total_n_features]].values, width=0.5)\n",
    "        plt.scatter(0 + ys, non_norm_subj_df[np.array(feature_names)[feature % total_n_features]].values, marker='o', zorder=10, color='grey', alpha=0.1)\n",
    "\n",
    "    # set weight of marker based on shap value\n",
    "    marker_size = np.abs(shap_values_over_time[-1, subj, feature]) * shap_value_base\n",
    "\n",
    "    # set z-order to make sure the scatter plot is on top\n",
    "    plt.scatter(0, median_norm_feature_df[feature_names[feature % total_n_features]], marker='o', s=marker_size, zorder=10, color=feature_color_dict[feature], alpha=0.8)\n",
    "    if (n % ncols) == 1:\n",
    "        if n <= len(selected_features) / 2:\n",
    "            ax.set_title(r\"$\\bf{Positive\\ features}$\" +f'\\n\\n{feature_names[feature % total_n_features]}')\n",
    "        else:\n",
    "            ax.set_title(r\"$\\bf{Negative\\ features}$\" + f'\\n\\n{feature_names[feature % total_n_features]}')\n",
    "\n",
    "    else:\n",
    "        ax.set_title(feature_names[feature % total_n_features])\n",
    "\n",
    "    ax.set_xlim(-1, 1)\n",
    "    ax.set_ylim(0, temp_pop_df.value.max() + temp_pop_df.value.max() / 5)\n",
    "    # turn off x axis\n",
    "    # ax.set_xticks([])\n",
    "\n",
    "    # add a legend for the shap value marker size on the bottom right of the last plot\n",
    "    if (n == len(selected_features) /2 - 1) & plot_legend:\n",
    "        legend_markers, legend_labels = ax.get_legend_handles_labels()\n",
    "        subj_value_1 = plt.scatter([], [], marker='o', s=shap_value_base/4, color=feature_color_dict[selected_features[0]], alpha=0.8)\n",
    "        subj_value_2 = plt.scatter([], [], marker='o', s=shap_value_base/8, color=feature_color_dict[selected_features[1]], alpha=0.8)\n",
    "        subj_value_3 = plt.scatter([], [], marker='o', s=shap_value_base/16, color=feature_color_dict[selected_features[2]], alpha=0.8)\n",
    "        subj_marker = (subj_value_1, subj_value_2, subj_value_3)\n",
    "        subj_labels = 'Subject feature value\\n(Size proportional to\\nweight in model)'\n",
    "        legend_markers.append(subj_marker)\n",
    "        legend_labels.append(subj_labels)\n",
    "\n",
    "        violin_patch1 = mpatches.Patch(color=feature_color_dict[selected_features[0]], alpha=0.1)\n",
    "        violin_patch2 = mpatches.Patch(color=feature_color_dict[selected_features[1]], alpha=0.1)\n",
    "        violin_patch3 = mpatches.Patch(color=feature_color_dict[selected_features[2]], alpha=0.1)\n",
    "        violin_plot_marker = (violin_patch1, violin_patch2, violin_patch3)\n",
    "        violin_plot_label = 'Distribution in\\ntraining population'\n",
    "        legend_markers.append(violin_plot_marker)\n",
    "        legend_labels.append(violin_plot_label)\n",
    "\n",
    "\n",
    "        if overlay_population_scatter:\n",
    "            # add a legend for the violin contour plots (population)\n",
    "            legend_markers.append(plt.scatter([], [], marker='o', s=10, color='grey', alpha=0.3))\n",
    "            legend_labels.append('Individual values in\\ntraining population')\n",
    "\n",
    "        # plot legend outside of right side of plot\n",
    "        # and avoid that markers are on the legend box border\n",
    "        ax.legend(legend_markers, legend_labels, fontsize=label_font_size,\n",
    "                  handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "                    bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.,\n",
    "                    handleheight=2, handlelength=4)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf785357",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:33:43.655851Z",
     "start_time": "2023-04-29T19:33:36.727824Z"
    }
   },
   "source": [
    "## Smooth shap values over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece8cb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:26:08.256235Z",
     "start_time": "2023-05-01T20:26:07.659497Z"
    }
   },
   "outputs": [],
   "source": [
    "smoothing_window = 15\n",
    "smoothed_shap_values_over_time = []\n",
    "for subj_idx in range(shap_values_over_time.shape[1]):\n",
    "    subj_smoothed_shap_values_over_time = []\n",
    "    for feature_idx in range(shap_values_over_time.shape[2]):\n",
    "        subj_smoothed_shap_values_over_time.append(smooth(shap_values_over_time[:, subj_idx, feature_idx], smoothing_window))\n",
    "    smoothed_shap_values_over_time.append(np.moveaxis(subj_smoothed_shap_values_over_time, 0, -1))\n",
    "smoothed_shap_values_over_time = np.moveaxis(smoothed_shap_values_over_time, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7de7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:09:49.512684Z",
     "start_time": "2023-04-29T19:09:49.493165Z"
    }
   },
   "source": [
    "## Plot evolution of prediction & explanation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6f47e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.115146Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_prevailing_features = False\n",
    "weigh_by_feature_value = False\n",
    "use_smoothed_shap_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3865374",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.115498Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_smoothed_shap_values:\n",
    "    working_shap_values = smoothed_shap_values_over_time\n",
    "else:\n",
    "    working_shap_values = shap_values_over_time\n",
    "\n",
    "cumulative_shap_values_over_time = np.array([working_shap_values[ts].sum(axis=1) for ts in range(n_time_steps)])\n",
    "\n",
    "# find index of 3 features with biggest positive shap impart & index of 3 features with biggest negative shap impart\n",
    "if overall_prevailing_features:\n",
    "    # prevailing features over cumulative time\n",
    "    selected_negative_features = cumulative_shap_values_over_time[:, subj].argsort()[:n_features][::-1]\n",
    "    selected_positive_features = cumulative_shap_values_over_time[:, subj].argsort()[-n_features:][::-1]\n",
    "else:\n",
    "    # prevailing features at last timepoint\n",
    "    selected_positive_features = np.squeeze(working_shap_values[-1])[subj].argsort()[-n_features:][::-1]\n",
    "    selected_negative_features = np.squeeze(working_shap_values[-1])[subj].argsort()[:n_features][::-1]\n",
    "\n",
    "selected_features = np.concatenate((selected_positive_features, selected_negative_features))\n",
    "\n",
    "fig3 = plt.figure(figsize=(15,10))\n",
    "\n",
    "k=0.1\n",
    "alpha=0.3\n",
    "\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "\n",
    "positive_color_palette = sns.color_palette(\"mako\", n_colors=len(selected_positive_features))\n",
    "negative_color_palette = sns.color_palette(\"flare_r\", n_colors=len(selected_negative_features))\n",
    "\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "ax = sns.lineplot(x=timestep_axis, y=subj_pred_over_ts, label='probability', linewidth = 2)\n",
    "\n",
    "\n",
    "pos_baseline = subj_pred_over_ts\n",
    "neg_baseline = subj_pred_over_ts\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "feature_color_dict = {}\n",
    "\n",
    "for i, feature in enumerate(selected_features):\n",
    "    subj_feature_shap_value_over_time = working_shap_values[:, subj, feature]\n",
    "    positive_portion = (subj_feature_shap_value_over_time > 0)\n",
    "    negative_portion = (subj_feature_shap_value_over_time < 0)\n",
    "\n",
    "    pos_function = subj_feature_shap_value_over_time.copy()\n",
    "    pos_function[negative_portion] = 0\n",
    "\n",
    "    neg_function = subj_feature_shap_value_over_time.copy()\n",
    "    neg_function[positive_portion] = 0\n",
    "\n",
    "    if feature in selected_positive_features:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    else:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    feature_color_dict[feature] = feature_color\n",
    "\n",
    "    positive_feature = pos_baseline + k * pos_function\n",
    "    if weigh_by_feature_value:\n",
    "        positive_feature *= X_test[subj, :, feature] / X_test[:, :, feature].max()\n",
    "    ax.fill_between(timestep_axis, pos_baseline, positive_feature, color=feature_color, alpha=alpha, label=feature_names[feature])\n",
    "    pos_baseline = positive_feature\n",
    "\n",
    "    negative_feature = neg_baseline + k * neg_function\n",
    "    if weigh_by_feature_value:\n",
    "        negative_feature *= X_test[subj, :, feature] / X_test[:, :, feature].max()\n",
    "    ax.fill_between(timestep_axis, negative_feature, neg_baseline, color=feature_color, alpha=alpha)\n",
    "    neg_baseline = negative_feature\n",
    "\n",
    "ax.legend(fontsize='x-large')\n",
    "\n",
    "ax.set_title(f'Predictions for subject {subj} of test set along time', fontsize=20)\n",
    "ax.set_xlabel('Time from admission (hours)', fontsize=15)\n",
    "ax.set_ylabel('Probability of favorable outcome', fontsize=15)\n",
    "\n",
    "# ax.set_ylim(0,1)\n",
    "\n",
    "plt.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce61446",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.115782Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig3.savefig(os.path.join(out_dir, 'prediction_over_time.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd808b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T19:35:45.664367Z",
     "start_time": "2023-05-01T19:35:45.655299Z"
    }
   },
   "source": [
    "## Plot selected features over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e322129d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.116006Z"
    }
   },
   "outputs": [],
   "source": [
    "fig4 = plt.figure(figsize=(15, 12))\n",
    "plt.subplots_adjust(hspace=0.2)\n",
    "plt.suptitle(\"Selected features\", fontsize=18, y=0.99, x=0.52, horizontalalignment='center')\n",
    "\n",
    "# set number of columns (use 3 to demonstrate the change)\n",
    "ncols = 3\n",
    "# calculate number of rows\n",
    "nrows = len(selected_features) // ncols + (len(selected_features) % ncols > 0)\n",
    "\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "# loop through the length of features and keep track of index\n",
    "for n, feature in enumerate(set(selected_features)):\n",
    "    # add a new subplot iteratively using nrows and cols\n",
    "    ax = plt.subplot(nrows, ncols, n + 1)\n",
    "\n",
    "    if feature in selected_positive_features:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    elif feature in selected_negative_features:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_negative_features_by_impact:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_positive_features_by_impact:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "\n",
    "    sns.lineplot(y=feature_names[feature % total_n_features], x=non_norm_subj_df.index.name, data=non_norm_subj_df.reset_index(), color=feature_color, ax=ax)\n",
    "\n",
    "    if (n % ncols) == 1:\n",
    "        if n <= len(selected_features) / 2:\n",
    "            ax.set_title(r\"$\\bf{Positive\\ features}$\" +f'\\n\\n{feature_names[feature % total_n_features]}')\n",
    "        else:\n",
    "            ax.set_title(r\"$\\bf{Negative\\ features}$\" + f'\\n\\n{feature_names[feature % total_n_features]}')\n",
    "    else:\n",
    "        ax.set_title(feature_names[feature])\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff1e2a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.116594Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig4.savefig(os.path.join(out_dir, 'features_over_time.png'), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71290a7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:07:02.902192Z",
     "start_time": "2023-05-01T20:07:02.124742Z"
    }
   },
   "source": [
    "## Identify features driving changes in prediction over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51dac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:53:45.591294Z",
     "start_time": "2023-05-01T20:53:45.168149Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.02\n",
    "n_features_selection = 0\n",
    "k=0.25\n",
    "alpha=0.3\n",
    "only_non_static_features = True\n",
    "use_smoothed_shap_values = True\n",
    "\n",
    "display_significant_slopes = True\n",
    "n_slope_steps = 5\n",
    "slope_threshold = 1.5 * threshold\n",
    "\n",
    "display_text_labels = True\n",
    "display_legend = True\n",
    "display_title = False\n",
    "plot_NIHSS_continuously = True\n",
    "ts_marker_level = 'baseline' # 'baseline' (marker on predicted probability function) or 'shap' (marker on SHAP value function)\n",
    "\n",
    "tick_label_size = 13\n",
    "label_font_size = 16\n",
    "\n",
    "if use_smoothed_shap_values:\n",
    "    working_shap_values = smoothed_shap_values_over_time\n",
    "else:\n",
    "    working_shap_values = shap_values_over_time\n",
    "\n",
    "# identify significant changes in prediction over time by a change in threshold X% in prediction\n",
    "significant_positive_timesteps = filter_consecutive_numbers(np.where(np.diff(subj_pred_over_ts) > threshold)[0])\n",
    "significant_negative_timesteps = filter_consecutive_numbers(np.where(np.diff(subj_pred_over_ts) < -threshold)[0])\n",
    "significant_timesteps = np.concatenate((significant_positive_timesteps, significant_negative_timesteps))\n",
    "\n",
    "non_norm_subj_df = feature_values_df[feature_values_df.case_admission_id_idx == subj].drop(columns=['case_admission_id_idx']).pivot(index='time_step', columns='feature', values='feature_value')\n",
    "\n",
    "# for each timestep, identify the feature that has the largest impact on the prediction\n",
    "if only_non_static_features:\n",
    "    # find non static columns in non normed df\n",
    "    non_static_features = np.where(non_norm_subj_df.std() > 0.01)[0]\n",
    "    if use_simplified_shap_values:\n",
    "       non_static_features = np.where(np.isin(feature_names, np.array(non_norm_subj_df.std()[non_norm_subj_df.std() > 0.01].index)))[0]\n",
    "    selected_positive_features_by_impact = np.diff(working_shap_values[:, subj, non_static_features], axis=0)[significant_positive_timesteps].argmax(axis=1)\n",
    "    selected_positive_features_by_impact = non_static_features[selected_positive_features_by_impact]\n",
    "    selected_negative_features_by_impact = np.diff(working_shap_values[:, subj, non_static_features], axis=0)[significant_negative_timesteps].argmin(axis=1)\n",
    "    selected_negative_features_by_impact = non_static_features[selected_negative_features_by_impact]\n",
    "else:\n",
    "    selected_positive_features_by_impact = np.diff(working_shap_values[:, subj], axis=0)[significant_positive_timesteps].argmax(axis=1)\n",
    "    selected_negative_features_by_impact = np.diff(working_shap_values[:, subj], axis=0)[significant_negative_timesteps].argmin(axis=1)\n",
    "\n",
    "selected_features_by_impact = np.concatenate((selected_positive_features_by_impact, selected_negative_features_by_impact))\n",
    "\n",
    "if display_significant_slopes:\n",
    "    # identify features that are driving the change in prediction over time more gentle slopes (then filter out consecutive timesteps)\n",
    "    significant_positive_slope = filter_consecutive_numbers(set(np.where((np.concatenate((subj_pred_over_ts[n_slope_steps:], np.zeros(n_slope_steps))) - subj_pred_over_ts)[:-n_slope_steps] > slope_threshold)[0]).difference(set(significant_positive_timesteps)))\n",
    "\n",
    "    significant_negative_slope = filter_consecutive_numbers(set(np.where((np.concatenate((subj_pred_over_ts[n_slope_steps:], np.zeros(n_slope_steps))) - subj_pred_over_ts)[:-n_slope_steps] < -slope_threshold)[0]).difference(set(significant_negative_timesteps)))\n",
    "\n",
    "    delta_shap_by_features = np.concatenate((working_shap_values[n_slope_steps:, subj, non_static_features], np.zeros((n_slope_steps, len(non_static_features))))) - working_shap_values[:, subj, non_static_features]\n",
    "    selected_positive_features_by_slope = delta_shap_by_features[:-n_slope_steps][significant_positive_slope].argmax(axis=1)\n",
    "    selected_positive_features_by_slope = non_static_features[selected_positive_features_by_slope]\n",
    "    selected_negative_features_by_slope = delta_shap_by_features[:-n_slope_steps][significant_negative_slope].argmin(axis=1)\n",
    "    selected_negative_features_by_slope = non_static_features[selected_negative_features_by_slope]\n",
    "\n",
    "    selected_features_by_impact = np.concatenate((selected_features_by_impact, selected_positive_features_by_slope, selected_negative_features_by_slope))\n",
    "    significant_timesteps = np.concatenate((significant_timesteps, significant_positive_slope, significant_negative_slope))\n",
    "    selected_positive_features_by_impact = np.concatenate((selected_positive_features_by_impact, selected_positive_features_by_slope))\n",
    "    selected_negative_features_by_impact = np.concatenate((selected_negative_features_by_impact, selected_negative_features_by_slope))\n",
    "\n",
    "if n_features_selection == 0:\n",
    "    selected_positive_features = np.array([])\n",
    "    selected_negative_features = np.array([])\n",
    "else:\n",
    "    selected_positive_features = working_shap_values[-1,subj].argsort()[-n_features:][::-1]\n",
    "    selected_negative_features = working_shap_values[-1,subj].argsort()[:n_features][::-1]\n",
    "\n",
    "selected_features = np.concatenate((selected_positive_features, selected_positive_features_by_impact, selected_negative_features, selected_negative_features_by_impact)).astype(int)\n",
    "\n",
    "fig3 = plt.figure(figsize=(15,10))\n",
    "\n",
    "\n",
    "positive_color_palette = sns.color_palette(\"mako\", n_colors=len(set(np.concatenate((selected_positive_features, selected_positive_features_by_impact)))))\n",
    "negative_color_palette = sns.color_palette(\"flare_r\", n_colors=len(set(np.concatenate((selected_negative_features, selected_negative_features_by_impact)))))\n",
    "\n",
    "# plot prediction over time\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "ax = sns.lineplot(x=timestep_axis, y=subj_pred_over_ts, label='Predicted probability', linewidth = 2)\n",
    "\n",
    "pos_baseline = subj_pred_over_ts\n",
    "neg_baseline = subj_pred_over_ts\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "feature_color_dict = {}\n",
    "for i, feature in enumerate(set(selected_features)):\n",
    "    subj_feature_shap_value_over_time = working_shap_values[:, subj, feature]\n",
    "    positive_portion = (subj_feature_shap_value_over_time > 0)\n",
    "    negative_portion = (subj_feature_shap_value_over_time < 0)\n",
    "\n",
    "\n",
    "    pos_function = subj_feature_shap_value_over_time.copy()\n",
    "    neg_function = subj_feature_shap_value_over_time.copy()\n",
    "    pos_function[negative_portion] = 0\n",
    "    neg_function[positive_portion] = 0\n",
    "\n",
    "    if feature in selected_features_by_impact:\n",
    "        important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "        # set value to zero before the significant timestep (except for NIHSS if plotting continuously)\n",
    "        if not np.logical_and(plot_NIHSS_continuously, feature_names[feature] == 'NIHSS'):\n",
    "            pos_function[:significant_timesteps[important_ts_idx][0] + 1] = 0\n",
    "            neg_function[:significant_timesteps[important_ts_idx][0] + 1] = 0\n",
    "\n",
    "    if feature in selected_positive_features:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    elif feature in selected_negative_features:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_negative_features_by_impact:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_positive_features_by_impact:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    feature_color_dict[feature] = feature_color\n",
    "\n",
    "    if np.any(pos_function):\n",
    "        positive_feature = pos_baseline + k * pos_function\n",
    "        ax.fill_between(timestep_axis , pos_baseline, positive_feature, color=feature_color, alpha=alpha)\n",
    "        pos_baseline = positive_feature\n",
    "\n",
    "    if np.any(neg_function):\n",
    "        negative_feature = neg_baseline + k * neg_function\n",
    "        ax.fill_between(timestep_axis, negative_feature, neg_baseline, color=feature_color, alpha=alpha)\n",
    "        neg_baseline = negative_feature\n",
    "\n",
    "    # add a legend entry for the feature fill\n",
    "    ax.scatter([], [], color=feature_color, alpha=alpha, label=feature_names[feature],marker=\"s\", s=200)\n",
    "\n",
    "\n",
    "# marking inflection points\n",
    "for feature in set(selected_features_by_impact):\n",
    "    important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "    for ts_idx in important_ts_idx:\n",
    "        # downward inflection point\n",
    "        if subj_pred_over_ts[significant_timesteps[ts_idx]] > subj_pred_over_ts[significant_timesteps[ts_idx] + 1]:\n",
    "            marker = 'v'\n",
    "            if ts_marker_level == 'shap':\n",
    "                marker_y_level = pos_baseline[significant_timesteps[ts_idx]] + 0.005\n",
    "            elif ts_marker_level == 'baseline':\n",
    "                marker_y_level = subj_pred_over_ts[significant_timesteps[ts_idx]] + 0.005\n",
    "            text_y_level = marker_y_level + 0.01\n",
    "        # upward inflection point\n",
    "        else:\n",
    "            marker = '^'\n",
    "            if ts_marker_level == 'shap':\n",
    "                marker_y_level = neg_baseline[significant_timesteps[ts_idx]] - 0.005\n",
    "            elif ts_marker_level == 'baseline':\n",
    "                marker_y_level = subj_pred_over_ts[significant_timesteps[ts_idx]] - 0.005\n",
    "            text_y_level = marker_y_level - 0.015\n",
    "\n",
    "        ax.scatter(significant_timesteps[ts_idx], marker_y_level, color=feature_color_dict[feature], s=100, marker=marker, alpha=1, edgecolors='white')\n",
    "        # insert a label on the plot\n",
    "        if display_text_labels:\n",
    "            ax.text(significant_timesteps[ts_idx]+ 0.01, text_y_level, feature_names[feature], fontsize=12, color='black')\n",
    "\n",
    "\n",
    "if display_title:\n",
    "    ax.set_title(f'Predictions for subject {subj} of test set along time', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('Time from admission (hours)', fontsize=label_font_size)\n",
    "ax.set_ylabel('Probability of favorable outcome', fontsize=label_font_size)\n",
    "ax.tick_params(axis='both', labelsize=tick_label_size)\n",
    "\n",
    "if display_legend:\n",
    "    legend_markers, legend_labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # shap value shades\n",
    "    shap_shades_markers = legend_markers[1:]\n",
    "    shap_shades_labels = legend_labels[1:]\n",
    "    legend_markers = [legend_markers[0]]\n",
    "    legend_labels = [legend_labels[0]]\n",
    "\n",
    "    # add a legend entry for the timestep markers\n",
    "    ts_marker_down = plt.scatter([], [], marker='v', color='grey', s=50, alpha=0.8)\n",
    "    ts_marker_up = plt.scatter([], [], marker='^', color='grey', s=50, alpha=0.8)\n",
    "    ts_label = 'Positive / Negative impact on inflection of prediction'\n",
    "    legend_markers.append((ts_marker_up, ts_marker_down))\n",
    "    legend_labels.append(ts_label)\n",
    "\n",
    "    # Add a subtitle for shape value shades\n",
    "    legend_markers.append('')\n",
    "    legend_labels.append('')\n",
    "    legend_markers.append('Weight & direction of influence on model prediction')\n",
    "    legend_labels.append('')\n",
    "\n",
    "    legend_markers += shap_shades_markers\n",
    "    legend_labels += shap_shades_labels\n",
    "\n",
    "    ax.legend(legend_markers, legend_labels, fontsize=label_font_size, title='Influence on model prediction', title_fontsize=label_font_size,\n",
    "              handler_map={tuple: HandlerTuple(ndivide=None), str: LegendTitle({'fontsize': label_font_size})})\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a50238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T20:55:26.689623Z",
     "start_time": "2023-05-01T20:55:26.476382Z"
    }
   },
   "outputs": [],
   "source": [
    "# fig3.savefig(os.path.join(out_dir, f'prediction_inflection_points_{subj}_nf{shap_values_over_time.shape[-1]}_k{k}_t{threshold}_text{int(display_text_labels)}.svg'), dpi=1200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc7c53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T19:39:18.997675Z",
     "start_time": "2023-05-01T19:39:18.994343Z"
    }
   },
   "source": [
    "Display shap value of given feature over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a72d3d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.158554Z"
    }
   },
   "outputs": [],
   "source": [
    "shap_val_df = pd.DataFrame(working_shap_values[:, subj, :], columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b3d1e4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.158891Z"
    }
   },
   "outputs": [],
   "source": [
    "shap_val_df['Total Cholesterol'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bebeb78",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.159144Z"
    }
   },
   "outputs": [],
   "source": [
    "shap_val_df['NIHSS'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c9d70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-29T19:50:31.949456Z",
     "start_time": "2023-04-29T19:50:31.922231Z"
    }
   },
   "source": [
    "Find interesting subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9898159",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.161639Z"
    }
   },
   "outputs": [],
   "source": [
    "# find subj with top difference in np.abs(predictions_over_time[0,:] - predictions_over_time[-1,:])\n",
    "top_10_diff_subj = np.argsort(np.abs(predictions_over_time[0,:] - predictions_over_time[-1,:]))[::-1][0:100]\n",
    "top_10_diff_subj_in_both_sets = np.intersect1d(top_10_diff_subj, subjs_in_both_test_sets)\n",
    "top_10_diff_subj_in_both_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e840d8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.162384Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions_over_time[-1, top_10_diff_subj_in_both_sets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c79309",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.162910Z"
    }
   },
   "outputs": [],
   "source": [
    "# get case admission idx with highes proBNP in feature_values_df\n",
    "feature_values_df.loc[(feature_values_df['case_admission_id_idx'].isin(top_10_diff_subj_in_both_sets)) & (feature_values_df.feature == 'proBNP')].sort_values(by='feature_value', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382aba7",
   "metadata": {},
   "source": [
    "Interesting subjects:\n",
    "- 54\n",
    "\n",
    "In both test sets:\n",
    "- 449, 32, 20\n",
    "\n",
    "Best: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314147e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-01T18:56:51.163629Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
