{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from prediction.outcome_prediction.LSTM.testing.shap_helper_functions import check_shap_version_compatibility\n",
    "from prediction.utils.scoring import precision, recall, matthews\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from prediction.outcome_prediction.data_loading.data_formatting import format_to_2d_table_with_time\n",
    "import torch as ch\n",
    "from torch import optim\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/preprocessed_features_01012023_233050.csv'\n",
    "labels_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/preprocessed_outcomes_01012023_233050.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/Users/jk1/Downloads/checkpoints_opsum_transformer_20230328_004215_cv_2/opsum_transformer_epoch=02_val_auroc=0.9227.ckpt'\n",
    "model_config_path = '/Users/jk1/Downloads/hyperopt_selected_transformer_20230328_004215.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = '3M mRS 0-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model config from json\n",
    "model_config = json.load(open(model_config_path, 'r'))\n",
    "model_config['outcome'] = outcome\n",
    "model_config['test_size'] = 0.2\n",
    "model_config['seed'] = 42\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "X, y = format_to_2d_table_with_time(feature_df_path=features_path, outcome_df_path=labels_path,\n",
    "                                    outcome=model_config['outcome'])\n",
    "\n",
    "n_time_steps = X.relative_sample_date_hourly_cat.max() + 1\n",
    "n_channels = X.sample_label.unique().shape[0]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prediction.outcome_prediction.data_loading.data_formatting import features_to_numpy, \\\n",
    "    link_patient_id_to_outcome, numpy_to_lookup_table\n",
    "\n",
    "# Reduce every patient to a single outcome (to avoid duplicates)\n",
    "all_pids_with_outcome = link_patient_id_to_outcome(y, model_config['outcome'])\n",
    "pid_train, pid_test, y_pid_train, y_pid_test = train_test_split(all_pids_with_outcome.patient_id.tolist(),\n",
    "                                                                all_pids_with_outcome.outcome.tolist(),\n",
    "                                                                stratify=all_pids_with_outcome.outcome.tolist(),\n",
    "                                                                test_size=model_config['test_size'],\n",
    "                                                                random_state=model_config['seed'])\n",
    "\n",
    "test_X_df = X[X.patient_id.isin(pid_test)]\n",
    "test_y_df = y[y.patient_id.isin(pid_test)]\n",
    "train_X_df = X[X.patient_id.isin(pid_train)]\n",
    "train_y_df = y[y.patient_id.isin(pid_train)]\n",
    "\n",
    "train_X_np = features_to_numpy(train_X_df,\n",
    "                               ['case_admission_id', 'relative_sample_date_hourly_cat', 'sample_label', 'value'])\n",
    "test_X_np = features_to_numpy(test_X_df,\n",
    "                              ['case_admission_id', 'relative_sample_date_hourly_cat', 'sample_label', 'value'])\n",
    "train_y_np = np.array([train_y_df[train_y_df.case_admission_id == cid].outcome.values[0] for cid in\n",
    "                       train_X_np[:, 0, 0, 0]]).astype('float32')\n",
    "test_y_np = np.array([test_y_df[test_y_df.case_admission_id == cid].outcome.values[0] for cid in\n",
    "                      test_X_np[:, 0, 0, 0]]).astype('float32')\n",
    "\n",
    "# create look-up table for case_admission_ids, sample_labels and relative_sample_date_hourly_cat\n",
    "test_features_lookup_table = numpy_to_lookup_table(test_X_np)\n",
    "train_features_lookup_table = numpy_to_lookup_table(train_X_np)\n",
    "\n",
    "# Remove the case_admission_id, sample_label, and time_step_label columns from the data\n",
    "test_X_np = test_X_np[:, :, :, -1].astype('float32')\n",
    "train_X_np = train_X_np[:, :, :, -1].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=model_config['seed'])\n",
    "\n",
    "splits = []\n",
    "for fold_pid_train_idx, fold_pid_val_idx in kfold.split(pid_train, y_pid_train):\n",
    "    fold_train_pidx = np.array(pid_train)[fold_pid_train_idx]\n",
    "    fold_val_pidx = np.array(pid_train)[fold_pid_val_idx]\n",
    "\n",
    "    fold_X_train_df = X.loc[X.patient_id.isin(fold_train_pidx)]\n",
    "    fold_y_train_df = y.loc[y.patient_id.isin(fold_train_pidx)]\n",
    "    fold_X_val_df = X.loc[X.patient_id.isin(fold_val_pidx)]\n",
    "    fold_y_val_df = y.loc[y.patient_id.isin(fold_val_pidx)]\n",
    "\n",
    "    fold_X_train = features_to_numpy(fold_X_train_df, ['case_admission_id', 'relative_sample_date_hourly_cat', 'sample_label', 'value'])\n",
    "    fold_X_val = features_to_numpy(fold_X_val_df, ['case_admission_id', 'relative_sample_date_hourly_cat', 'sample_label', 'value'])\n",
    "\n",
    "    fold_y_train = np.array([fold_y_train_df[fold_y_train_df.case_admission_id == cid].outcome.values[0] for cid in fold_X_train[:, 0, 0, 0]]).astype('float32')\n",
    "    fold_y_val = np.array([fold_y_val_df[fold_y_val_df.case_admission_id == cid].outcome.values[0] for cid in fold_X_val[:, 0, 0, 0]]).astype('float32')\n",
    "\n",
    "    fold_X_train = fold_X_train[:, :, :, -1].astype('float32')\n",
    "    fold_X_val = fold_X_val[:, :, :, -1].astype('float32')\n",
    "\n",
    "    splits.append((fold_X_train, fold_X_val, fold_y_train, fold_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr, wd, train_noise):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.train_noise = train_noise\n",
    "        self.criterion = ch.nn.BCEWithLogitsLoss()\n",
    "        self.train_accuracy = Accuracy(task='binary')\n",
    "        self.train_accuracy_epoch = Accuracy(task='binary')\n",
    "        self.val_accuracy_epoch = Accuracy(task='binary')\n",
    "        self.train_auroc = AUROC(task=\"binary\")\n",
    "        self.val_auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx, mode='train'):\n",
    "        x, y = batch\n",
    "        if self.train_noise != 0:\n",
    "            x = x + ch.randn_like(x) * self.train_noise\n",
    "        predictions = self.model(x).squeeze().ravel()\n",
    "        y = y.unsqueeze(1).repeat(1, x.shape[1]).ravel()\n",
    "        loss = self.criterion(predictions, y.float()).ravel()\n",
    "        self.train_accuracy(predictions.ravel(), y.ravel())\n",
    "        self.train_accuracy_epoch(predictions.ravel(), y.ravel())\n",
    "        # self.train_auroc(ch.sigmoid(predictions.ravel()), y.ravel())\n",
    "        # self.log(\"train_auroc\", self.train_auroc, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        # self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        # self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"train_acc_epoch\", self.train_accuracy_epoch, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch, batch_idx, mode='train'):\n",
    "        x, y = batch\n",
    "        predictions = self.model(x).squeeze().ravel()\n",
    "        y = y.unsqueeze(1).repeat(1, x.shape[1]).ravel()\n",
    "        loss = self.criterion(predictions, y.float()).ravel()\n",
    "        self.val_auroc(ch.sigmoid(predictions.ravel()), y.ravel())\n",
    "        # self.val_accuracy_epoch(predictions.ravel(), y.ravel())\n",
    "        self.log(\"val_auroc\", self.val_auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"val_accuracy\", self.val_accuracy_epoch, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.model(x).squeeze()\n",
    "        return predictions\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        # optimizer = optim.SGD(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "\n",
    "        return [optimizer], [optim.lr_scheduler.ExponentialLR(optimizer, 0.99)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def prepare_dataset(X_train, X_val, y_train, y_val, balanced=False):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, 84)).reshape(X_train.shape)\n",
    "    if balanced:\n",
    "        X_train_neg = X_train[y_train == 0]\n",
    "        X_train_pos = X_train[np.random.choice(np.where(y_train==1)[0], X_train_neg.shape[0])]\n",
    "        X_train = np.concatenate([X_train_neg, X_train_pos])\n",
    "        y_train = np.concatenate([np.zeros(X_train_neg.shape[0]), np.ones(X_train_pos.shape[0])])\n",
    "    X_val = scaler.transform(X_val.reshape(-1, 84)).reshape(X_val.shape)\n",
    "    # train_dataset = TensorDataset(ch.from_numpy(X_train).cuda(), ch.from_numpy(y_train.astype(np.int32)).cuda())\n",
    "    # val_dataset = TensorDataset(ch.from_numpy(X_val).cuda(), ch.from_numpy(y_val.astype(np.int32)).cuda())\n",
    "    train_dataset = TensorDataset(ch.from_numpy(X_train), ch.from_numpy(y_train.astype(np.int32)))\n",
    "    val_dataset = TensorDataset(ch.from_numpy(X_val), ch.from_numpy(y_val.astype(np.int32)))\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.outcome_prediction.Transformer.architecture import OPSUMTransformer\n",
    "\n",
    "ff_factor = 2\n",
    "ff_dim = ff_factor * model_config['model_dim']\n",
    "pos_encode_factor = 1\n",
    "\n",
    "model = OPSUMTransformer(\n",
    "            input_dim=84,\n",
    "            num_layers=int(model_config['num_layers']),\n",
    "            model_dim=int(model_config['model_dim']),\n",
    "            dropout=int(model_config['dropout']),\n",
    "            ff_dim=int(ff_dim),\n",
    "            num_heads=int(model_config['num_head']),\n",
    "            num_classes=1,\n",
    "            max_dim=500,\n",
    "            pos_encode_factor=pos_encode_factor\n",
    "        )\n",
    "module = LitModel(model, model_config['lr'], model_config['weight_decay'], model_config['train_noise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = LitModel.load_from_checkpoint(checkpoint_path=model_path, model=model, lr=model_config['lr'], wd=model_config['weight_decay'], train_noise=model_config['train_noise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.eval()\n",
    "with ch.no_grad():\n",
    "    y_hat = saved_model.predict_step(ch.from_numpy(test_X_np))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fold_train_X, _, model_fold_train_y, _ = splits[int(model_config['best_cv_fold'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = prepare_dataset(model_fold_train_X, test_X_np, model_fold_train_y, test_y_np, balanced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.outcome_prediction.Transformer.utils import DictLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "logger = DictLogger(0)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024)\n",
    "trainer = pl.Trainer(accelerator='cpu', devices=1, max_epochs=1000, gradient_clip_val=model_config['grad_clip_value'], logger=logger)\n",
    "predictions = trainer.predict(saved_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = ch.sigmoid(y_hat)\n",
    "y_hat = y_hat[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(test_y_np, y_hat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model\n",
    "model_y_pred_train = np.where(model_y_train > 0.5, 1, 0).astype('float32')\n",
    "model_acc_train = accuracy_score(y_train, model_y_pred_train)\n",
    "model_precision_train = precision(y_train, model_y_pred_train.astype(float)).numpy()\n",
    "model_sn_train = recall(y_train, model_y_pred_train).numpy()\n",
    "model_auc_train = roc_auc_score(y_train, model_y_train)\n",
    "model_mcc_train = matthews_corrcoef(y_train, model_y_pred_train)\n",
    "model_sp_train = specificity(y_train, model_y_pred_train).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_hat_std = np.std(y_hat.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(y_hat_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(y_hat_std), np.max(y_hat_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_sigm = ch.sigmoid(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Use the training data for deep explainer => can use fewer instances\n",
    "explainer = shap.DeepExplainer(saved_model.model, ch.from_numpy(train_X_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the testing instances (can use fewer instances)\n",
    "# explaining each prediction requires 2 * background dataset size runs\n",
    "shap_values = explainer.shap_values(ch.from_numpy(test_X_np[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
