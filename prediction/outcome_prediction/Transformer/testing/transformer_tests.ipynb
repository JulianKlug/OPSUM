{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:21:19.307585Z",
     "start_time": "2023-04-16T12:21:19.296070Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from prediction.utils.scoring import precision, recall, matthews\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from prediction.outcome_prediction.Transformer.utils.utils import prepare_dataset, DictLogger\n",
    "import torch as ch\n",
    "from torch import optim\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:16:19.046357Z",
     "start_time": "2023-04-16T12:16:19.039795Z"
    }
   },
   "outputs": [],
   "source": [
    "features_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/preprocessed_features_01012023_233050.csv'\n",
    "labels_path = '/Users/jk1/temp/opsum_prepro_output/gsu_prepro_01012023_233050/preprocessed_outcomes_01012023_233050.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:17:55.224938Z",
     "start_time": "2023-04-16T12:17:55.217333Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = '/Users/jk1/Downloads/checkpoints_opsum_transformer_20230328_004215_cv_2/opsum_transformer_epoch=02_val_auroc=0.9227.ckpt'\n",
    "model_config_path = '/Users/jk1/Downloads/hyperopt_selected_transformer_20230328_004215.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:17:56.435842Z",
     "start_time": "2023-04-16T12:17:56.424123Z"
    }
   },
   "outputs": [],
   "source": [
    "outcome = '3M mRS 0-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:17:56.885828Z",
     "start_time": "2023-04-16T12:17:56.780861Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model config from json\n",
    "model_config = json.load(open(model_config_path, 'r'))\n",
    "model_config['outcome'] = outcome\n",
    "model_config['test_size'] = 0.2\n",
    "model_config['seed'] = 42\n",
    "model_config['n_splits'] = 5\n",
    "model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:20:42.310755Z",
     "start_time": "2023-04-16T12:17:59.218855Z"
    }
   },
   "outputs": [],
   "source": [
    "from prediction.outcome_prediction.data_loading.data_loader import load_data\n",
    "\n",
    "pids, train_data, test_data, train_splits, test_features_lookup_table = load_data(features_path, labels_path, outcome, model_config['test_size'], model_config['n_splits'], model_config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:23:37.717724Z",
     "start_time": "2023-04-16T12:23:37.556131Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train_data\n",
    "X_test, y_test = test_data\n",
    "\n",
    "# Prepare train dataset\n",
    "train_dataset, _ = prepare_dataset((X_train, X_test, y_train, y_test),\n",
    "                                              balanced=model_config['balanced'],\n",
    "                                              rescale=True,\n",
    "                                              use_gpu=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=X_train.shape[0], shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:23:45.275037Z",
     "start_time": "2023-04-16T12:23:45.244506Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "train_sample, _ = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:22:14.888707Z",
     "start_time": "2023-04-16T12:22:14.880357Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-16T12:24:03.325215Z",
     "start_time": "2023-04-16T12:24:03.314418Z"
    }
   },
   "outputs": [],
   "source": [
    "background = train_sample[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model, lr, wd, train_noise):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.train_noise = train_noise\n",
    "        self.criterion = ch.nn.BCEWithLogitsLoss()\n",
    "        self.train_accuracy = Accuracy(task='binary')\n",
    "        self.train_accuracy_epoch = Accuracy(task='binary')\n",
    "        self.val_accuracy_epoch = Accuracy(task='binary')\n",
    "        self.train_auroc = AUROC(task=\"binary\")\n",
    "        self.val_auroc = AUROC(task=\"binary\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx, mode='train'):\n",
    "        x, y = batch\n",
    "        if self.train_noise != 0:\n",
    "            x = x + ch.randn_like(x) * self.train_noise\n",
    "        predictions = self.model(x).squeeze().ravel()\n",
    "        y = y.unsqueeze(1).repeat(1, x.shape[1]).ravel()\n",
    "        loss = self.criterion(predictions, y.float()).ravel()\n",
    "        self.train_accuracy(predictions.ravel(), y.ravel())\n",
    "        self.train_accuracy_epoch(predictions.ravel(), y.ravel())\n",
    "        # self.train_auroc(ch.sigmoid(predictions.ravel()), y.ravel())\n",
    "        # self.log(\"train_auroc\", self.train_auroc, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        # self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        # self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"train_acc_epoch\", self.train_accuracy_epoch, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,batch, batch_idx, mode='train'):\n",
    "        x, y = batch\n",
    "        predictions = self.model(x).squeeze().ravel()\n",
    "        y = y.unsqueeze(1).repeat(1, x.shape[1]).ravel()\n",
    "        loss = self.criterion(predictions, y.float()).ravel()\n",
    "        self.val_auroc(ch.sigmoid(predictions.ravel()), y.ravel())\n",
    "        # self.val_accuracy_epoch(predictions.ravel(), y.ravel())\n",
    "        self.log(\"val_auroc\", self.val_auroc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        # self.log(\"val_accuracy\", self.val_accuracy_epoch, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        predictions = self.model(x).squeeze()\n",
    "        return predictions\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        # optimizer = optim.SGD(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "\n",
    "        return [optimizer], [optim.lr_scheduler.ExponentialLR(optimizer, 0.99)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def prepare_dataset(X_train, X_val, y_train, y_val, balanced=False):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train.reshape(-1, 84)).reshape(X_train.shape)\n",
    "    if balanced:\n",
    "        X_train_neg = X_train[y_train == 0]\n",
    "        X_train_pos = X_train[np.random.choice(np.where(y_train==1)[0], X_train_neg.shape[0])]\n",
    "        X_train = np.concatenate([X_train_neg, X_train_pos])\n",
    "        y_train = np.concatenate([np.zeros(X_train_neg.shape[0]), np.ones(X_train_pos.shape[0])])\n",
    "    X_val = scaler.transform(X_val.reshape(-1, 84)).reshape(X_val.shape)\n",
    "    # train_dataset = TensorDataset(ch.from_numpy(X_train).cuda(), ch.from_numpy(y_train.astype(np.int32)).cuda())\n",
    "    # val_dataset = TensorDataset(ch.from_numpy(X_val).cuda(), ch.from_numpy(y_val.astype(np.int32)).cuda())\n",
    "    train_dataset = TensorDataset(ch.from_numpy(X_train), ch.from_numpy(y_train.astype(np.int32)))\n",
    "    val_dataset = TensorDataset(ch.from_numpy(X_val), ch.from_numpy(y_val.astype(np.int32)))\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.outcome_prediction.Transformer.architecture import OPSUMTransformer\n",
    "\n",
    "ff_factor = 2\n",
    "ff_dim = ff_factor * model_config['model_dim']\n",
    "pos_encode_factor = 1\n",
    "\n",
    "model = OPSUMTransformer(\n",
    "            input_dim=84,\n",
    "            num_layers=int(model_config['num_layers']),\n",
    "            model_dim=int(model_config['model_dim']),\n",
    "            dropout=int(model_config['dropout']),\n",
    "            ff_dim=int(ff_dim),\n",
    "            num_heads=int(model_config['num_head']),\n",
    "            num_classes=1,\n",
    "            max_dim=500,\n",
    "            pos_encode_factor=pos_encode_factor\n",
    "        )\n",
    "module = LitModel(model, model_config['lr'], model_config['weight_decay'], model_config['train_noise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = LitModel.load_from_checkpoint(checkpoint_path=model_path, model=model, lr=model_config['lr'], wd=model_config['weight_decay'], train_noise=model_config['train_noise'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.eval()\n",
    "with ch.no_grad():\n",
    "    y_hat = saved_model.predict_step(ch.from_numpy(test_X_np))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fold_train_X, _, model_fold_train_y, _ = splits[int(model_config['best_cv_fold'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = prepare_dataset(model_fold_train_X, test_X_np, model_fold_train_y, test_y_np, balanced=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.outcome_prediction.Transformer.utils import DictLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "logger = DictLogger(0)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024)\n",
    "trainer = pl.Trainer(accelerator='cpu', devices=1, max_epochs=1000, gradient_clip_val=model_config['grad_clip_value'], logger=logger)\n",
    "predictions = trainer.predict(saved_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = ch.sigmoid(y_hat)\n",
    "y_hat = y_hat[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(test_y_np, y_hat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "model\n",
    "model_y_pred_train = np.where(model_y_train > 0.5, 1, 0).astype('float32')\n",
    "model_acc_train = accuracy_score(y_train, model_y_pred_train)\n",
    "model_precision_train = precision(y_train, model_y_pred_train.astype(float)).numpy()\n",
    "model_sn_train = recall(y_train, model_y_pred_train).numpy()\n",
    "model_auc_train = roc_auc_score(y_train, model_y_train)\n",
    "model_mcc_train = matthews_corrcoef(y_train, model_y_pred_train)\n",
    "model_sp_train = specificity(y_train, model_y_pred_train).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_hat_std = np.std(y_hat.numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(y_hat_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(y_hat_std), np.max(y_hat_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_sigm = ch.sigmoid(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Use the training data for deep explainer => can use fewer instances\n",
    "explainer = shap.DeepExplainer(saved_model.model, ch.from_numpy(train_X_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the testing instances (can use fewer instances)\n",
    "# explaining each prediction requires 2 * background dataset size runs\n",
    "shap_values = explainer.shap_values(ch.from_numpy(test_X_np[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
