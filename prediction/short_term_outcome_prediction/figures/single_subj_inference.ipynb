{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from random import randint\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch as ch\n",
    "from tqdm import tqdm\n",
    "from matplotlib.lines import Line2D\n",
    "from prediction.utils.utils import smooth, filter_consecutive_numbers\n",
    "from prediction.utils.visualisation_helper_functions import reverse_normalisation_for_subj, LegendTitle\n",
    "from preprocessing.preprocessing_tools.normalisation.reverse_normalisation import reverse_normalisation\n",
    "from prediction.utils.shap_helper_functions import check_shap_version_compatibility\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prediction.outcome_prediction.data_loading.data_formatting import format_to_2d_table_with_time, \\\n",
    "    link_patient_id_to_outcome, features_to_numpy, numpy_to_lookup_table\n",
    "from prediction.utils.visualisation_helper_functions import density_jitter\n",
    "from matplotlib.legend_handler import HandlerTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c709ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_path = '/Users/jk1/temp/opsum_end/training/hyperopt/xgb_gridsearch/xgb_gs_20250513_154517/checkpoints_short_opsum_xgb_20250518_001112_cv_1/shap_explanations_over_time/tree_explainer_shap_values_over_ts.pkl'\n",
    "test_data_path = '/Users/jk1/temp/opsum_end/preprocessing/gsu_Extraction_20220815_prepro_09052025_220520/early_neurological_deterioration_train_data_splits/test_data_early_neurological_deterioration_ts0.8_rs42_ns5.pth'\n",
    "cat_encoding_path = '/Users/jk1/temp/opsum_end/preprocessing/gsu_Extraction_20220815_prepro_09052025_220520/logs_09052025_220520/categorical_variable_encoding.csv'\n",
    "\n",
    "normalisation_parameters_path = '/Users/jk1/temp/opsum_end/preprocessing/gsu_Extraction_20220815_prepro_09052025_220520/logs_09052025_220520/normalisation_parameters.csv'\n",
    "predictions_path = '/Users/jk1/temp/opsum_end/testing/test_gt_and_pred_cv_1.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2048c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time_steps = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the shap values\n",
    "with open(os.path.join(shap_values_path), 'rb') as handle:\n",
    "    original_shap_values = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_last_timestep = False\n",
    "if only_last_timestep:\n",
    "    # use predictions from last timestep (as it also produces output for other timesteps)\n",
    "    shap_values = [original_shap_values[-1]]\n",
    "\n",
    "else:\n",
    "    shap_values = [np.array([original_shap_values[i] for i in range(len(original_shap_values))]).swapaxes(0, 1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c057fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalisation_parameters_df = pd.read_csv(normalisation_parameters_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36007db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(predictions_path, 'rb') as handle:\n",
    "    gt_over_time, predictions_over_time = pickle.load(handle)\n",
    "    \n",
    "gt_over_time = gt_over_time.reshape(-1, n_time_steps)\n",
    "predictions_over_time = predictions_over_time.reshape(-1, n_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653bf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, full_y_test = ch.load(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6167385",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_np = X_test[:, :, :, -1].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a31bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test[0, 0, :, 2]\n",
    "\n",
    "# features, avg_features, min_features, max_features\n",
    "# create feature names for avg_features, min_features, max_features\n",
    "avg_features = [f'avg_{i}' for i in features]\n",
    "min_features = [f'min_{i}' for i in features]\n",
    "max_features = [f'max_{i}' for i in features]\n",
    "# combine all feature names\n",
    "aggregated_feature_names = features.tolist() + avg_features + min_features + max_features + ['base_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85120f01",
   "metadata": {},
   "source": [
    "## Create working data frame\n",
    "Join data in a common dataframe with shap values and feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f77f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_categorical_encoding = True\n",
    "pool_hourly_split_values = True\n",
    "only_keep_current_value_shap = True # (do not use aggregated features)\n",
    "# pool_time_aggregated_features = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171749d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_df = pd.DataFrame()\n",
    "for ts in tqdm(range(n_time_steps)):\n",
    "    ts_shap_values_df = pd.DataFrame(data=shap_values[:, ts], columns = np.array(aggregated_feature_names))\n",
    "    ts_shap_values_df = ts_shap_values_df.reset_index()\n",
    "    ts_shap_values_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)\n",
    "    ts_shap_values_df = ts_shap_values_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='shap_value')\n",
    "    ts_shap_values_df['time_step'] = ts\n",
    "    shap_values_df = shap_values_df.append(ts_shap_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ad983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not use aggregated features (avg, min, max)\n",
    "if only_keep_current_value_shap:\n",
    "    shap_values_df = shap_values_df[shap_values_df['feature'].isin(features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123dab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_values_df = pd.DataFrame()\n",
    "for subj_idx in tqdm(range(test_X_np.shape[0])):\n",
    "    subj_feature_values_df = pd.DataFrame(data=test_X_np[subj_idx, :, :], columns = np.array(features))\n",
    "    subj_feature_values_df = reverse_normalisation_for_subj(subj_feature_values_df, normalisation_parameters_df)\n",
    "    subj_feature_values_df = subj_feature_values_df.reset_index()\n",
    "    subj_feature_values_df.rename(columns={'index': 'time_step'}, inplace=True)\n",
    "    subj_feature_values_df['case_admission_id_idx'] = subj_idx\n",
    "    subj_feature_values_df = subj_feature_values_df.melt(id_vars=['case_admission_id_idx', 'time_step'],  var_name='feature', value_name='feature_value')\n",
    "    feature_values_df = feature_values_df.append(subj_feature_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c3c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_aggregation_func = 'sum' # 'median' or 'sum' (but sum makes mokes more sense)\n",
    "\n",
    "if reverse_categorical_encoding:\n",
    "    cat_encoding_df = pd.read_csv(cat_encoding_path)\n",
    "    for i in tqdm(range(len(cat_encoding_df))):\n",
    "        cat_basename = cat_encoding_df.sample_label[i].lower().replace(' ', '_')\n",
    "        cat_item_list = cat_encoding_df.other_categories[i].replace('[', '').replace(']', '').replace('\\'', '').split(', ')\n",
    "        cat_item_list = [cat_basename + '_' + item.replace(' ', '_').lower() for item in cat_item_list]\n",
    "        for cat_item_idx, cat_item in enumerate(cat_item_list):\n",
    "            #  retrieve the dominant category for this subject (0 being default category)\n",
    "            feature_values_df.loc[feature_values_df.feature == cat_item, 'feature_value'] *= cat_item_idx + 1\n",
    "            feature_values_df.loc[feature_values_df.feature == cat_item, 'feature'] = cat_encoding_df.sample_label[i]\n",
    "            feature_values_df = feature_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).sum().reset_index()\n",
    "\n",
    "            shap_values_df.loc[shap_values_df.feature == cat_item, 'feature'] = cat_encoding_df.sample_label[i]\n",
    "            # sum the shap and feature values for each subject\n",
    "            if shap_aggregation_func:\n",
    "                shap_values_df = shap_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).sum().reset_index()\n",
    "            else:\n",
    "                shap_values_df = shap_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).median().reset_index()\n",
    "\n",
    "    # give a numerical encoding to the categorical features\n",
    "    cat_to_numerical_encoding = {\n",
    "        'Prestroke disability (Rankin)': {0:0, 1:3, 2:4, 3:2, 4:1, 5:5},\n",
    "        'categorical_onset_to_admission_time': {0:2, 1:1, 2:0, 3:3, 4:5, 5:4},\n",
    "        'categorical_IVT': {0:2, 1:3, 2:4, 3:1, 4:0},\n",
    "        'categorical_IAT': {0:1, 1:2, 2:3, 3:0}\n",
    "    }\n",
    "\n",
    "    for cat_feature, cat_encoding in cat_to_numerical_encoding.items():\n",
    "        feature_values_df.loc[feature_values_df.feature == cat_feature, 'feature_value'] = feature_values_df.loc[feature_values_df.feature == cat_feature, 'feature_value'].map(cat_encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For features that are downsampled to hourly values, pool the values (median, min, max)\n",
    "if pool_hourly_split_values:\n",
    "    hourly_split_features = ['NIHSS', 'systolic_blood_pressure', 'diastolic_blood_pressure', 'mean_blood_pressure', 'heart_rate', 'respiratory_rate', 'temperature', 'oxygen_saturation']\n",
    "    for feature in tqdm(hourly_split_features):\n",
    "        shap_values_df.loc[shap_values_df.feature.str.contains(feature), 'feature'] = (feature[0].upper() + feature[1:]).replace('_', ' ')\n",
    "        if shap_aggregation_func == 'median':\n",
    "            shap_values_df = shap_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).median().reset_index()\n",
    "        elif shap_aggregation_func == 'sum':\n",
    "            shap_values_df = shap_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).sum().reset_index()\n",
    "\n",
    "        feature_values_df.loc[feature_values_df.feature.str.contains(feature), 'feature'] = (feature[0].upper() + feature[1:]).replace('_', ' ')\n",
    "        feature_values_df = feature_values_df.groupby(['case_admission_id_idx', 'feature', 'time_step']).median().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc45d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace feature names with their english names\n",
    "feature_to_english_name_correspondence_path = os.path.join(os.path.dirname(os.path.abspath('__file__')),\n",
    "                                                           'preprocessing/preprocessing_tools/feature_name_to_english_name_correspondence.xlsx')\n",
    "feature_to_english_name_correspondence = pd.read_excel(feature_to_english_name_correspondence_path)\n",
    "\n",
    "for feature in shap_values_df.feature.unique():\n",
    "    if feature in feature_to_english_name_correspondence.feature_name.values:\n",
    "        shap_values_df.loc[shap_values_df.feature == feature, 'feature'] = feature_to_english_name_correspondence[feature_to_english_name_correspondence.feature_name == feature].english_name.values[0]\n",
    "\n",
    "for feature in feature_values_df.feature.unique():\n",
    "    if feature in feature_to_english_name_correspondence.feature_name.values:\n",
    "        feature_values_df.loc[feature_values_df.feature == feature, 'feature'] = feature_to_english_name_correspondence[feature_to_english_name_correspondence.feature_name == feature].english_name.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_simplified_shap_values = True\n",
    "\n",
    "if use_simplified_shap_values:\n",
    "    shap_values_over_time = []\n",
    "    for ts in tqdm(range(n_time_steps)):\n",
    "        subj_values_over_time = []\n",
    "        for subj in range(len(test_X_np)):\n",
    "            subj_values_over_time.append(shap_values_df[(shap_values_df.case_admission_id_idx == subj) & (shap_values_df.time_step == ts)].shap_value.values)\n",
    "        shap_values_over_time.append(np.array(subj_values_over_time))\n",
    "    shap_values_over_time = np.array(shap_values_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_feature_names = shap_values_df.feature.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window = 15\n",
    "smoothed_shap_values_over_time = []\n",
    "for subj_idx in range(shap_values_over_time.shape[1]):\n",
    "    subj_smoothed_shap_values_over_time = []\n",
    "    for feature_idx in range(shap_values_over_time.shape[2]):\n",
    "        subj_smoothed_shap_values_over_time.append(smooth(shap_values_over_time[:, subj_idx, feature_idx], smoothing_window))\n",
    "    smoothed_shap_values_over_time.append(np.moveaxis(subj_smoothed_shap_values_over_time, 0, -1))\n",
    "smoothed_shap_values_over_time = np.moveaxis(smoothed_shap_values_over_time, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eeed8e",
   "metadata": {},
   "source": [
    "## Choose subject and load prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = randint(0, len(test_X_np))\n",
    "subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_pred_over_ts = predictions_over_time[subj, :]\n",
    "subj_gt_over_ts = gt_over_time[subj, :]\n",
    "subj_pred_over_ts.shape, subj_gt_over_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867eb98a",
   "metadata": {},
   "source": [
    "## Plot overall subject prediction & explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ebdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_legend = True\n",
    "plot_shap_direction_label = True\n",
    "\n",
    "tick_label_size = 13\n",
    "label_font_size = 15\n",
    "\n",
    "fig1 = plt.figure(figsize=(10,7.5))\n",
    "ax1 = fig1.add_subplot(111)\n",
    "\n",
    "# plot a bar plot showing impact of most important features on the prediction across all n_time_steps\n",
    "# find index of 3 features with biggest positive shap impart\n",
    "selected_positive_features = np.squeeze(shap_values_over_time[-1])[subj].argsort()[-n_features:][::-1]\n",
    "# find index of 3 features with biggest negative shap impart\n",
    "selected_negative_features = np.squeeze(shap_values_over_time[-1])[subj].argsort()[:n_features][::-1]\n",
    "selected_features = np.concatenate((selected_positive_features, selected_negative_features))\n",
    "\n",
    "\n",
    "non_norm_subj_df = feature_values_df[feature_values_df.case_admission_id_idx == subj].drop(columns=['case_admission_id_idx']).pivot(index='time_step', columns='feature', values='feature_value')\n",
    "non_norm_subj_df.loc[non_norm_subj_df['IVT timing'] == 1, 'IVT timing'] = '91-270min'\n",
    "\n",
    "y_labels = []\n",
    "for fidx, feature in enumerate(selected_features):\n",
    "    feature_name = reduced_feature_names[feature]\n",
    "    feature_value = non_norm_subj_df[feature_name].iloc[-1]\n",
    "    if feature_name in ['NIHSS', 'Prestroke disability (Rankin)']:\n",
    "        y_labels.append(f'{feature_name}: {int(feature_value)}')\n",
    "    elif type(feature_value) == str:\n",
    "        y_labels.append(f'{feature_name}: {feature_value}')\n",
    "    else:\n",
    "        y_labels.append(f'{feature_name}: {feature_value:.1f}')\n",
    "\n",
    "ax = sns.barplot(y=y_labels, x=np.squeeze(shap_values_over_time[-1])[subj][selected_features], palette=\"RdBu_r\", ax=ax1)\n",
    "\n",
    "# Plot additional explanation with the shap value X axis\n",
    "if plot_shap_direction_label:\n",
    "    x_ticks_coordinates = plt.xticks()[0]\n",
    "    x_ticks_labels = [item.get_text() for item in plt.xticks()[1]]\n",
    "    # let x tick label be the coordinate with 2 decimals\n",
    "    x_ticks_labels = [f'{x_ticks_coordinate:.02f}' for x_ticks_coordinate in x_ticks_coordinates]\n",
    "\n",
    "    x_ticks_labels[0] = f'Toward better\\noutcome'\n",
    "    x_ticks_labels[-1] = f'Toward worse\\noutcome'\n",
    "    plt.xticks(x_ticks_coordinates, x_ticks_labels)\n",
    "\n",
    "ax.tick_params(axis='y', labelsize=label_font_size)\n",
    "ax.tick_params(axis='x', labelsize=tick_label_size)\n",
    "\n",
    "ax.set_xlabel('SHAP Value \\n(impact on model output)', fontsize=label_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e4300",
   "metadata": {},
   "source": [
    "## Plot evolution of prediction & explanation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b76cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_prevailing_features = False\n",
    "weigh_by_feature_value = False\n",
    "use_smoothed_shap_values = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56723721",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_smoothed_shap_values:\n",
    "    working_shap_values = smoothed_shap_values_over_time\n",
    "else:\n",
    "    working_shap_values = shap_values_over_time\n",
    "\n",
    "cumulative_shap_values_over_time = np.array([working_shap_values[ts].sum(axis=1) for ts in range(n_time_steps)])\n",
    "\n",
    "# find index of 3 features with biggest positive shap impart & index of 3 features with biggest negative shap impart\n",
    "if overall_prevailing_features:\n",
    "    # prevailing features over cumulative time\n",
    "    selected_negative_features = cumulative_shap_values_over_time[:, subj].argsort()[:n_features][::-1]\n",
    "    selected_positive_features = cumulative_shap_values_over_time[:, subj].argsort()[-n_features:][::-1]\n",
    "else:\n",
    "    # prevailing features at last timepoint\n",
    "    selected_positive_features = np.squeeze(working_shap_values[-1])[subj].argsort()[-n_features:][::-1]\n",
    "    selected_negative_features = np.squeeze(working_shap_values[-1])[subj].argsort()[:n_features][::-1]\n",
    "\n",
    "selected_features = np.concatenate((selected_positive_features, selected_negative_features))\n",
    "\n",
    "fig3 = plt.figure(figsize=(15,10))\n",
    "\n",
    "k=0.1\n",
    "alpha=0.3\n",
    "\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "\n",
    "positive_color_palette = sns.color_palette(\"mako\", n_colors=len(selected_positive_features))\n",
    "negative_color_palette = sns.color_palette(\"flare_r\", n_colors=len(selected_negative_features))\n",
    "\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "ax = sns.lineplot(x=timestep_axis, y=subj_pred_over_ts, label='probability', linewidth = 2)\n",
    "\n",
    "\n",
    "pos_baseline = subj_pred_over_ts\n",
    "neg_baseline = subj_pred_over_ts\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "feature_color_dict = {}\n",
    "\n",
    "for i, feature in enumerate(selected_features):\n",
    "    subj_feature_shap_value_over_time = working_shap_values[:, subj, feature]\n",
    "    positive_portion = (subj_feature_shap_value_over_time > 0)\n",
    "    negative_portion = (subj_feature_shap_value_over_time < 0)\n",
    "\n",
    "    pos_function = subj_feature_shap_value_over_time.copy()\n",
    "    pos_function[negative_portion] = 0\n",
    "\n",
    "    neg_function = subj_feature_shap_value_over_time.copy()\n",
    "    neg_function[positive_portion] = 0\n",
    "\n",
    "    if feature in selected_positive_features:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    else:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    feature_color_dict[feature] = feature_color\n",
    "\n",
    "    positive_feature = pos_baseline + k * pos_function\n",
    "    if weigh_by_feature_value:\n",
    "        positive_feature *= X_test[subj, :, feature] / X_test[:, :, feature].max()\n",
    "    ax.fill_between(timestep_axis, pos_baseline, positive_feature, color=feature_color, alpha=alpha, label=reduced_feature_names[feature])\n",
    "    pos_baseline = positive_feature\n",
    "\n",
    "    negative_feature = neg_baseline + k * neg_function\n",
    "    if weigh_by_feature_value:\n",
    "        negative_feature *= X_test[subj, :, feature] / X_test[:, :, feature].max()\n",
    "    ax.fill_between(timestep_axis, negative_feature, neg_baseline, color=feature_color, alpha=alpha)\n",
    "    neg_baseline = negative_feature\n",
    "\n",
    "ax.legend(fontsize='x-large')\n",
    "\n",
    "ax.set_title(f'Predictions for subject {subj} of test set along time', fontsize=20)\n",
    "ax.set_xlabel('Time from admission (hours)', fontsize=15)\n",
    "ax.set_ylabel('Probability of favorable outcome', fontsize=15)\n",
    "\n",
    "# ax.set_ylim(0,1)\n",
    "\n",
    "plt.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc570ccc",
   "metadata": {},
   "source": [
    "## Identify features driving changes in prediction over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.035\n",
    "n_features_selection = 0\n",
    "k=0.25\n",
    "alpha=0.3\n",
    "n_features = 2\n",
    "only_non_static_features = True\n",
    "use_smoothed_shap_values = True\n",
    "plot_ground_truth = True\n",
    "\n",
    "display_significant_slopes = True\n",
    "n_slope_steps = 5\n",
    "slope_threshold = 1.5 * threshold\n",
    "\n",
    "display_text_labels = True\n",
    "display_legend = True\n",
    "display_title = False\n",
    "plot_NIHSS_continuously = True\n",
    "ts_marker_level = 'baseline' # 'baseline' (marker on predicted probability function) or 'shap' (marker on SHAP value function)\n",
    "\n",
    "fig3 = plt.figure(figsize=(15,10))\n",
    "\n",
    "tick_label_size = 13\n",
    "label_font_size = 16\n",
    "\n",
    "if use_smoothed_shap_values:\n",
    "    working_shap_values = smoothed_shap_values_over_time\n",
    "else:\n",
    "    working_shap_values = shap_values_over_time\n",
    "\n",
    "# identify significant changes in prediction over time by a change in threshold X% in prediction\n",
    "significant_positive_timesteps = filter_consecutive_numbers(np.where(np.diff(subj_pred_over_ts) > threshold)[0])\n",
    "significant_negative_timesteps = filter_consecutive_numbers(np.where(np.diff(subj_pred_over_ts) < -threshold)[0])\n",
    "significant_timesteps = np.concatenate((significant_positive_timesteps, significant_negative_timesteps))\n",
    "\n",
    "non_norm_subj_df = feature_values_df[feature_values_df.case_admission_id_idx == subj].drop(columns=['case_admission_id_idx']).pivot(index='time_step', columns='feature', values='feature_value')\n",
    "\n",
    "# for each timestep, identify the feature that has the largest impact on the prediction\n",
    "if only_non_static_features:\n",
    "    # find non static columns in non normed df\n",
    "    non_static_features = np.where(non_norm_subj_df.std() > 0.01)[0]\n",
    "    if use_simplified_shap_values:\n",
    "       non_static_features = np.where(np.isin(reduced_feature_names, np.array(non_norm_subj_df.std()[non_norm_subj_df.std() > 0.01].index)))[0]\n",
    "    selected_positive_features_by_impact = np.diff(working_shap_values[:, subj, non_static_features], axis=0)[significant_positive_timesteps].argmax(axis=1)\n",
    "    selected_positive_features_by_impact = non_static_features[selected_positive_features_by_impact]\n",
    "    selected_negative_features_by_impact = np.diff(working_shap_values[:, subj, non_static_features], axis=0)[significant_negative_timesteps].argmin(axis=1)\n",
    "    selected_negative_features_by_impact = non_static_features[selected_negative_features_by_impact]\n",
    "else:\n",
    "    selected_positive_features_by_impact = np.diff(working_shap_values[:, subj], axis=0)[significant_positive_timesteps].argmax(axis=1)\n",
    "    selected_negative_features_by_impact = np.diff(working_shap_values[:, subj], axis=0)[significant_negative_timesteps].argmin(axis=1)\n",
    "\n",
    "selected_features_by_impact = np.concatenate((selected_positive_features_by_impact, selected_negative_features_by_impact))\n",
    "\n",
    "if display_significant_slopes:\n",
    "    # identify features that are driving the change in prediction over time more gentle slopes (then filter out consecutive timesteps)\n",
    "    significant_positive_slope = filter_consecutive_numbers(set(np.where((np.concatenate((subj_pred_over_ts[n_slope_steps:], np.zeros(n_slope_steps))) - subj_pred_over_ts)[:-n_slope_steps] > slope_threshold)[0]).difference(set(significant_positive_timesteps)))\n",
    "\n",
    "    significant_negative_slope = filter_consecutive_numbers(set(np.where((np.concatenate((subj_pred_over_ts[n_slope_steps:], np.zeros(n_slope_steps))) - subj_pred_over_ts)[:-n_slope_steps] < -slope_threshold)[0]).difference(set(significant_negative_timesteps)))\n",
    "\n",
    "    delta_shap_by_features = np.concatenate((working_shap_values[n_slope_steps:, subj, non_static_features], np.zeros((n_slope_steps, len(non_static_features))))) - working_shap_values[:, subj, non_static_features]\n",
    "    selected_positive_features_by_slope = delta_shap_by_features[:-n_slope_steps][significant_positive_slope].argmax(axis=1)\n",
    "    selected_positive_features_by_slope = non_static_features[selected_positive_features_by_slope]\n",
    "    selected_negative_features_by_slope = delta_shap_by_features[:-n_slope_steps][significant_negative_slope].argmin(axis=1)\n",
    "    selected_negative_features_by_slope = non_static_features[selected_negative_features_by_slope]\n",
    "\n",
    "    selected_features_by_impact = np.concatenate((selected_features_by_impact, selected_positive_features_by_slope, selected_negative_features_by_slope))\n",
    "    significant_timesteps = np.concatenate((significant_timesteps, significant_positive_slope, significant_negative_slope))\n",
    "    selected_positive_features_by_impact = np.concatenate((selected_positive_features_by_impact, selected_positive_features_by_slope))\n",
    "    selected_negative_features_by_impact = np.concatenate((selected_negative_features_by_impact, selected_negative_features_by_slope))\n",
    "\n",
    "if n_features_selection == 0:\n",
    "    selected_positive_features = np.array([])\n",
    "    selected_negative_features = np.array([])\n",
    "else:\n",
    "    selected_positive_features = working_shap_values[-1,subj].argsort()[-n_features:][::-1]\n",
    "    selected_negative_features = working_shap_values[-1,subj].argsort()[:n_features][::-1]\n",
    "\n",
    "selected_features = np.concatenate((selected_positive_features, selected_positive_features_by_impact, selected_negative_features, selected_negative_features_by_impact)).astype(int)\n",
    "\n",
    "\n",
    "positive_color_palette = sns.color_palette(\"mako\", n_colors=len(set(np.concatenate((selected_positive_features, selected_positive_features_by_impact)))))\n",
    "negative_color_palette = sns.color_palette(\"flare_r\", n_colors=len(set(np.concatenate((selected_negative_features, selected_negative_features_by_impact)))))\n",
    "\n",
    "# plot prediction over time\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "ax = sns.lineplot(x=timestep_axis, y=subj_pred_over_ts, label='Predicted probability', linewidth = 2)\n",
    "\n",
    "# plot ground truth over time\n",
    "if plot_ground_truth:\n",
    "    # get changes in subj_gt_over_ts\n",
    "    changes_in_gt = np.diff(subj_gt_over_ts, prepend=0)\n",
    "    # pair np.where(changes_in_gt == -1) and np.where(changes_in_gt == 1\n",
    "    change_pairs = list(zip(np.where(changes_in_gt == 1)[0], np.where(changes_in_gt == -1)[0]))\n",
    "    for change_pair in change_pairs:\n",
    "        # mark a thick horizontal red line at the ground truth value (-1)\n",
    "        ax.plot([change_pair[0], change_pair[1]], [0, 0], color=\"#7b002c\", linewidth=10, alpha=0.8)\n",
    "        ax.text(np.mean(change_pair), 0 + 0.02, '6h to END', horizontalalignment='center', verticalalignment='center', fontsize=tick_label_size)\n",
    "\n",
    "pos_baseline = subj_pred_over_ts\n",
    "neg_baseline = subj_pred_over_ts\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "feature_color_dict = {}\n",
    "for i, feature in enumerate(set(selected_features)):\n",
    "    subj_feature_shap_value_over_time = working_shap_values[:, subj, feature]\n",
    "    positive_portion = (subj_feature_shap_value_over_time > 0)\n",
    "    negative_portion = (subj_feature_shap_value_over_time < 0)\n",
    "\n",
    "\n",
    "    pos_function = subj_feature_shap_value_over_time.copy()\n",
    "    neg_function = subj_feature_shap_value_over_time.copy()\n",
    "    pos_function[negative_portion] = 0\n",
    "    neg_function[positive_portion] = 0\n",
    "\n",
    "    if feature in selected_features_by_impact:\n",
    "        important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "        # set value to zero before the significant timestep (except for NIHSS if plotting continuously)\n",
    "        if not np.logical_and(plot_NIHSS_continuously, reduced_feature_names[feature] == 'NIHSS'):\n",
    "            pos_function[:significant_timesteps[important_ts_idx][0] + 1] = 0\n",
    "            neg_function[:significant_timesteps[important_ts_idx][0] + 1] = 0\n",
    "\n",
    "    if feature in selected_positive_features:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    elif feature in selected_negative_features:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_negative_features_by_impact:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_positive_features_by_impact:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    feature_color_dict[feature] = feature_color\n",
    "\n",
    "    if np.any(pos_function):\n",
    "        positive_feature = pos_baseline + k * pos_function\n",
    "        ax.fill_between(timestep_axis , pos_baseline, positive_feature, color=feature_color, alpha=alpha)\n",
    "        pos_baseline = positive_feature\n",
    "\n",
    "    if np.any(neg_function):\n",
    "        negative_feature = neg_baseline + k * neg_function\n",
    "        ax.fill_between(timestep_axis, negative_feature, neg_baseline, color=feature_color, alpha=alpha)\n",
    "        neg_baseline = negative_feature\n",
    "\n",
    "    # add a legend entry for the feature fill\n",
    "    ax.scatter([], [], color=feature_color, alpha=alpha, label=reduced_feature_names[feature],marker=\"s\", s=200)\n",
    "\n",
    "\n",
    "# marking inflection points\n",
    "for feature in set(selected_features_by_impact):\n",
    "    important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "    for ts_idx in important_ts_idx:\n",
    "        # downward inflection point\n",
    "        if subj_pred_over_ts[significant_timesteps[ts_idx]] > subj_pred_over_ts[significant_timesteps[ts_idx] + 1]:\n",
    "            marker = 'v'\n",
    "            if ts_marker_level == 'shap':\n",
    "                marker_y_level = pos_baseline[significant_timesteps[ts_idx]] + 0.005\n",
    "            elif ts_marker_level == 'baseline':\n",
    "                marker_y_level = subj_pred_over_ts[significant_timesteps[ts_idx]] + 0.005\n",
    "            text_y_level = marker_y_level + 0.01\n",
    "        # upward inflection point\n",
    "        else:\n",
    "            marker = '^'\n",
    "            if ts_marker_level == 'shap':\n",
    "                marker_y_level = neg_baseline[significant_timesteps[ts_idx]] - 0.005\n",
    "            elif ts_marker_level == 'baseline':\n",
    "                marker_y_level = subj_pred_over_ts[significant_timesteps[ts_idx]] - 0.005\n",
    "            text_y_level = marker_y_level - 0.015\n",
    "\n",
    "        ax.scatter(significant_timesteps[ts_idx], marker_y_level, color=feature_color_dict[feature], s=100, marker=marker, alpha=1, edgecolors='white')\n",
    "        # insert a label on the plot\n",
    "        if display_text_labels:\n",
    "            # rotate the text label by 45 degrees (up if downward inflection, down if upward inflection)\n",
    "            # ax.text(significant_timesteps[ts_idx]+ 0.01, text_y_level, reduced_feature_names[feature], fontsize=12, color='black')\n",
    "            if marker == 'v':\n",
    "                ax.text(significant_timesteps[ts_idx] + 0.01, text_y_level, reduced_feature_names[feature], fontsize=12, color='black', rotation=45)\n",
    "            else:\n",
    "                ax.text(significant_timesteps[ts_idx] + 0.01, text_y_level, reduced_feature_names[feature], fontsize=12, color='black', rotation=-45,\n",
    "                        horizontalalignment='left', verticalalignment='top', rotation_mode='anchor')\n",
    "\n",
    "\n",
    "if display_title:\n",
    "    ax.set_title(f'Predictions for subject {subj} of test set along time', fontsize=20)\n",
    "\n",
    "ax.set_xlabel('Time from admission (hours)', fontsize=label_font_size)\n",
    "ax.set_ylabel('Probability of favorable outcome', fontsize=label_font_size)\n",
    "ax.tick_params(axis='both', labelsize=tick_label_size)\n",
    "\n",
    "if display_legend:\n",
    "    legend_markers, legend_labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    # shap value shades\n",
    "    shap_shades_markers = legend_markers[1:]\n",
    "    shap_shades_labels = legend_labels[1:]\n",
    "    legend_markers = [legend_markers[0]]\n",
    "    legend_labels = [legend_labels[0]]\n",
    "\n",
    "    # add a legend entry for the timestep markers\n",
    "    ts_marker_down = plt.scatter([], [], marker='v', color='grey', s=50, alpha=0.8)\n",
    "    ts_marker_up = plt.scatter([], [], marker='^', color='grey', s=50, alpha=0.8)\n",
    "    ts_label = 'Positive / Negative impact on inflection of prediction'\n",
    "    legend_markers.append((ts_marker_up, ts_marker_down))\n",
    "    legend_labels.append(ts_label)\n",
    "\n",
    "    # Add a subtitle for shape value shades\n",
    "    legend_markers.append('')\n",
    "    legend_labels.append('')\n",
    "    legend_markers.append('Weight & direction of influence on model prediction')\n",
    "    legend_labels.append('')\n",
    "\n",
    "    legend_markers += shap_shades_markers\n",
    "    legend_labels += shap_shades_labels\n",
    "\n",
    "    ax.legend(legend_markers, legend_labels, fontsize=label_font_size, title='Influence on model prediction', title_fontsize=label_font_size,\n",
    "              handler_map={tuple: HandlerTuple(ndivide=None), str: LegendTitle({'fontsize': label_font_size})}, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# remove top and right spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig5, (ax_main, axr) = plt.subplots(\n",
    "#     ncols=2,\n",
    "#     figsize=(25, 10),\n",
    "#     gridspec_kw=dict(width_ratios=[1.5, 1], wspace=0.1),\n",
    "# )\n",
    "\n",
    "fig5, (ax_main, axr) = plt.subplots(\n",
    "    ncols=1,\n",
    "    nrows=2,\n",
    "    sharex=True,\n",
    "    figsize=(15, 15),\n",
    "    gridspec_kw=dict(hspace=0.05, height_ratios=[1, 0.5]),\n",
    ")\n",
    "\n",
    "\n",
    "#### FIGURE 3 ####\n",
    "threshold = 0.05\n",
    "n_features_selection = 0\n",
    "k=0.25\n",
    "alpha=0.3\n",
    "only_non_static_features = True\n",
    "use_smoothed_shap_values = True\n",
    "\n",
    "display_significant_slopes = True\n",
    "n_slope_steps = 5\n",
    "slope_threshold = 1.5 * threshold\n",
    "\n",
    "display_text_labels = False\n",
    "display_legend = True\n",
    "display_title = False\n",
    "plot_NIHSS_continuously = True\n",
    "ts_marker_level = 'shap' # 'baseline' (marker on predicted probability function) or 'shap' (marker on SHAP value function)\n",
    "\n",
    "tick_label_size = 13\n",
    "label_font_size = 16\n",
    "\n",
    "if use_smoothed_shap_values:\n",
    "    working_shap_values = smoothed_shap_values_over_time\n",
    "else:\n",
    "    working_shap_values = shap_values_over_time\n",
    "\n",
    "# identify significant changes in prediction over time by a change in threshold X% in prediction\n",
    "significant_positive_timesteps = filter_consecutive_numbers(np.where(np.diff(subj_pred_over_ts) > threshold)[0])\n",
    "significant_negative_timesteps = filter_consecutive_numbers(np.where(np.diff(subj_pred_over_ts) < -threshold)[0])\n",
    "significant_timesteps = np.concatenate((significant_positive_timesteps, significant_negative_timesteps))\n",
    "\n",
    "non_norm_subj_df = feature_values_df[feature_values_df.case_admission_id_idx == subj].drop(columns=['case_admission_id_idx']).pivot(index='time_step', columns='feature', values='feature_value')\n",
    "\n",
    "# for each timestep, identify the feature that has the largest impact on the prediction\n",
    "if only_non_static_features:\n",
    "    # find non static columns in non normed df\n",
    "    non_static_features = np.where(non_norm_subj_df.std() > 0.01)[0]\n",
    "    if use_simplified_shap_values:\n",
    "       non_static_features = np.where(np.isin(reduced_feature_names, np.array(non_norm_subj_df.std()[non_norm_subj_df.std() > 0.01].index)))[0]\n",
    "    selected_positive_features_by_impact = np.diff(working_shap_values[:, subj, non_static_features], axis=0)[significant_positive_timesteps].argmax(axis=1)\n",
    "    selected_positive_features_by_impact = non_static_features[selected_positive_features_by_impact]\n",
    "    selected_negative_features_by_impact = np.diff(working_shap_values[:, subj, non_static_features], axis=0)[significant_negative_timesteps].argmin(axis=1)\n",
    "    selected_negative_features_by_impact = non_static_features[selected_negative_features_by_impact]\n",
    "else:\n",
    "    selected_positive_features_by_impact = np.diff(working_shap_values[:, subj], axis=0)[significant_positive_timesteps].argmax(axis=1)\n",
    "    selected_negative_features_by_impact = np.diff(working_shap_values[:, subj], axis=0)[significant_negative_timesteps].argmin(axis=1)\n",
    "\n",
    "selected_features_by_impact = np.concatenate((selected_positive_features_by_impact, selected_negative_features_by_impact))\n",
    "\n",
    "if display_significant_slopes:\n",
    "    # identify features that are driving the change in prediction over time more gentle slopes (then filter out consecutive timesteps)\n",
    "    significant_positive_slope = filter_consecutive_numbers(set(np.where((np.concatenate((subj_pred_over_ts[n_slope_steps:], np.zeros(n_slope_steps))) - subj_pred_over_ts)[:-n_slope_steps] > slope_threshold)[0]).difference(set(significant_positive_timesteps)))\n",
    "\n",
    "    significant_negative_slope = filter_consecutive_numbers(set(np.where((np.concatenate((subj_pred_over_ts[n_slope_steps:], np.zeros(n_slope_steps))) - subj_pred_over_ts)[:-n_slope_steps] < -slope_threshold)[0]).difference(set(significant_negative_timesteps)))\n",
    "\n",
    "    delta_shap_by_features = np.concatenate((working_shap_values[n_slope_steps:, subj, non_static_features], np.zeros((n_slope_steps, len(non_static_features))))) - working_shap_values[:, subj, non_static_features]\n",
    "    selected_positive_features_by_slope = delta_shap_by_features[:-n_slope_steps][significant_positive_slope].argmax(axis=1)\n",
    "    selected_positive_features_by_slope = non_static_features[selected_positive_features_by_slope]\n",
    "    selected_negative_features_by_slope = delta_shap_by_features[:-n_slope_steps][significant_negative_slope].argmin(axis=1)\n",
    "    selected_negative_features_by_slope = non_static_features[selected_negative_features_by_slope]\n",
    "\n",
    "    selected_features_by_impact = np.concatenate((selected_features_by_impact, selected_positive_features_by_slope, selected_negative_features_by_slope))\n",
    "    significant_timesteps = np.concatenate((significant_timesteps, significant_positive_slope, significant_negative_slope))\n",
    "    selected_positive_features_by_impact = np.concatenate((selected_positive_features_by_impact, selected_positive_features_by_slope))\n",
    "    selected_negative_features_by_impact = np.concatenate((selected_negative_features_by_impact, selected_negative_features_by_slope))\n",
    "\n",
    "if n_features_selection == 0:\n",
    "    selected_positive_features = np.array([])\n",
    "    selected_negative_features = np.array([])\n",
    "else:\n",
    "    selected_positive_features = working_shap_values[-1,subj].argsort()[-n_features:][::-1]\n",
    "    selected_negative_features = working_shap_values[-1,subj].argsort()[:n_features][::-1]\n",
    "\n",
    "selected_features = np.concatenate((selected_positive_features, selected_positive_features_by_impact, selected_negative_features, selected_negative_features_by_impact)).astype(int)\n",
    "\n",
    "\n",
    "positive_color_palette = sns.color_palette(\"mako\", n_colors=len(set(np.concatenate((selected_positive_features, selected_positive_features_by_impact)))))\n",
    "negative_color_palette = sns.color_palette(\"flare_r\", n_colors=len(set(np.concatenate((selected_negative_features, selected_negative_features_by_impact)))))\n",
    "\n",
    "# plot prediction over time\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "sns.lineplot(x=timestep_axis, y=subj_pred_over_ts, label='Predicted probability', linewidth = 2, ax=ax_main)\n",
    "\n",
    "pos_baseline = subj_pred_over_ts\n",
    "neg_baseline = subj_pred_over_ts\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "feature_color_dict = {}\n",
    "for i, feature in enumerate(set(selected_features)):\n",
    "    subj_feature_shap_value_over_time = working_shap_values[:, subj, feature]\n",
    "    positive_portion = (subj_feature_shap_value_over_time > 0)\n",
    "    negative_portion = (subj_feature_shap_value_over_time < 0)\n",
    "\n",
    "\n",
    "    pos_function = subj_feature_shap_value_over_time.copy()\n",
    "    neg_function = subj_feature_shap_value_over_time.copy()\n",
    "    pos_function[negative_portion] = 0\n",
    "    neg_function[positive_portion] = 0\n",
    "\n",
    "    if feature in selected_features_by_impact:\n",
    "        important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "        # set value to zero before the significant timestep (except for NIHSS if plotting continuously)\n",
    "        if not np.logical_and(plot_NIHSS_continuously, reduced_feature_names[feature] == 'NIHSS'):\n",
    "            pos_function[:significant_timesteps[important_ts_idx][0] + 1] = 0\n",
    "            neg_function[:significant_timesteps[important_ts_idx][0] + 1] = 0\n",
    "\n",
    "    if feature in selected_positive_features:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    elif feature in selected_negative_features:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_negative_features_by_impact:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_positive_features_by_impact:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    feature_color_dict[feature] = feature_color\n",
    "\n",
    "    if np.any(pos_function):\n",
    "        positive_feature = pos_baseline + k * pos_function\n",
    "        ax_main.fill_between(timestep_axis , pos_baseline, positive_feature, color=feature_color, alpha=alpha)\n",
    "        pos_baseline = positive_feature\n",
    "\n",
    "    if np.any(neg_function):\n",
    "        negative_feature = neg_baseline + k * neg_function\n",
    "        ax_main.fill_between(timestep_axis, negative_feature, neg_baseline, color=feature_color, alpha=alpha)\n",
    "        neg_baseline = negative_feature\n",
    "\n",
    "    # add a legend entry for the feature fill\n",
    "    ax_main.scatter([], [], color=feature_color, alpha=alpha, label=reduced_feature_names[feature],marker=\"s\", s=200)\n",
    "\n",
    "\n",
    "# marking inflection points\n",
    "for feature in set(selected_features_by_impact):\n",
    "    important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "    for ts_idx in important_ts_idx:\n",
    "        # downward inflection point\n",
    "        if subj_pred_over_ts[significant_timesteps[ts_idx]] > subj_pred_over_ts[significant_timesteps[ts_idx] + 1]:\n",
    "            marker = 'v'\n",
    "            if ts_marker_level == 'shap':\n",
    "                marker_y_level = pos_baseline[significant_timesteps[ts_idx]] + 0.005\n",
    "            elif ts_marker_level == 'baseline':\n",
    "                marker_y_level = subj_pred_over_ts[significant_timesteps[ts_idx]] + 0.005\n",
    "            text_y_level = marker_y_level + 0.01\n",
    "        # upward inflection point\n",
    "        else:\n",
    "            marker = '^'\n",
    "            if ts_marker_level == 'shap':\n",
    "                marker_y_level = neg_baseline[significant_timesteps[ts_idx]] - 0.005\n",
    "            elif ts_marker_level == 'baseline':\n",
    "                marker_y_level = subj_pred_over_ts[significant_timesteps[ts_idx]] - 0.005\n",
    "            text_y_level = marker_y_level - 0.015\n",
    "\n",
    "        ax_main.scatter(significant_timesteps[ts_idx], marker_y_level, color=feature_color_dict[feature], s=100, marker=marker, alpha=1, edgecolors='white')\n",
    "        # insert a label on the plot\n",
    "        if display_text_labels:\n",
    "            ax_main.text(significant_timesteps[ts_idx]+ 0.01, text_y_level, reduced_feature_names[feature], fontsize=12, color='black')\n",
    "\n",
    "\n",
    "if display_title:\n",
    "    ax_main.set_title(f'Predictions for subject {subj} of test set along time', fontsize=20)\n",
    "\n",
    "ax_main.set_xlabel('Time from admission (hours)', fontsize=label_font_size)\n",
    "ax_main.set_ylabel('Probability of favorable outcome', fontsize=label_font_size)\n",
    "ax_main.tick_params(axis='both', labelsize=tick_label_size)\n",
    "\n",
    "if display_legend:\n",
    "    legend_markers, legend_labels = ax_main.get_legend_handles_labels()\n",
    "\n",
    "    # shap value shades\n",
    "    shap_shades_markers = legend_markers[1:]\n",
    "    shap_shades_labels = legend_labels[1:]\n",
    "    legend_markers = [legend_markers[0]]\n",
    "    legend_labels = [legend_labels[0]]\n",
    "\n",
    "    # add a legend entry for the timestep markers\n",
    "    ts_marker_down = plt.scatter([], [], marker='v', color='grey', s=50, alpha=0.8)\n",
    "    ts_marker_up = plt.scatter([], [], marker='^', color='grey', s=50, alpha=0.8)\n",
    "    ts_label = 'Positive / Negative impact on inflection of prediction'\n",
    "    legend_markers.append((ts_marker_up, ts_marker_down))\n",
    "    legend_labels.append(ts_label)\n",
    "\n",
    "    # Add a subtitle for shape value shades\n",
    "    legend_markers.append('')\n",
    "    legend_labels.append('')\n",
    "    legend_markers.append('Weight & direction of influence on model prediction')\n",
    "    legend_labels.append('')\n",
    "\n",
    "    legend_markers += shap_shades_markers\n",
    "    legend_labels += shap_shades_labels\n",
    "\n",
    "    ax_main.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., fontsize=label_font_size, title='Influence on model prediction', title_fontsize=label_font_size,\n",
    "              handler_map={tuple: HandlerTuple(ndivide=None), str: LegendTitle({'fontsize': label_font_size})})\n",
    "\n",
    "\n",
    "##### Figure 4 #####\n",
    "\n",
    "smooth_values = True\n",
    "plot_legend = True\n",
    "smoothing_window = 2\n",
    "plot_normalized_features = False\n",
    "display_inflection_markers = True\n",
    "\n",
    "legend_markers, legend_labels = [], []\n",
    "\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "\n",
    "if plot_normalized_features:\n",
    "    # plot all normalized features in the background (grey)\n",
    "    static_feature_val0_count = 0\n",
    "    static_feature_val1_count = 0\n",
    "    for norm_feature_idx in range(test_X_np.shape[-1]):\n",
    "        if np.all(test_X_np[subj, :, norm_feature_idx] == 0):\n",
    "            if static_feature_val0_count == 0:\n",
    "                static_feature_val0_count += 1\n",
    "            else:\n",
    "                continue\n",
    "        elif np.all(test_X_np[subj, :, norm_feature_idx] == 1):\n",
    "            if static_feature_val1_count == 0:\n",
    "                static_feature_val1_count += 1\n",
    "            else:\n",
    "                continue\n",
    "        axr.plot(timestep_axis, test_X_np[subj, :, norm_feature_idx], color='grey', alpha=0.075)\n",
    "\n",
    "# plot selected non static features in color on top\n",
    "non_norm_subj_df = feature_values_df[feature_values_df.case_admission_id_idx == subj].drop(columns=['case_admission_id_idx']).pivot(index='time_step', columns='feature', values='feature_value')\n",
    "twin_xs = []\n",
    "for fidx, feature in enumerate(set(selected_features_by_impact)):\n",
    "    if plot_normalized_features or fidx != 0:\n",
    "        twin_xs.append(axr.twinx())\n",
    "    else:\n",
    "        # first feature is plotted on the left axis if no normalized features are displayed\n",
    "        twin_xs.append(axr)\n",
    "    feature_name = reduced_feature_names[feature]\n",
    "    feature_color = feature_color_dict[feature]\n",
    "    if smooth_values:\n",
    "        non_norm_subj_df = non_norm_subj_df.rolling(window=smoothing_window, min_periods=1, center=True).mean()\n",
    "\n",
    "    sns.lineplot(y=feature_name, x=non_norm_subj_df.index.name, data=non_norm_subj_df.reset_index(), color=feature_color, ax=twin_xs[-1])\n",
    "\n",
    "    if plot_normalized_features:\n",
    "        twin_xs[-1].spines.right.set_position((\"axes\", 1 + 0.1 * fidx))\n",
    "    elif fidx != 0:\n",
    "        twin_xs[-1].spines.right.set_position((\"axes\", 1 + 0.1 * (fidx - 1)))\n",
    "\n",
    "    twin_xs[-1].set_ylabel(feature_name, fontsize=tick_label_size)\n",
    "    # twin_xs[-1].yaxis.label.set_color(feature_color)\n",
    "    twin_xs[-1].tick_params(axis='y', colors=feature_color, labelsize=tick_label_size -1 )\n",
    "\n",
    "    # add a legend entry for the feature\n",
    "    legend_markers.append(plt.Line2D([0,0],[0,0], color=feature_color))\n",
    "    legend_labels.append(feature_name)\n",
    "\n",
    "    twin_xs[-1].grid(False)\n",
    "\n",
    "    if display_inflection_markers:\n",
    "        important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "        for ts_idx in important_ts_idx:\n",
    "            timestep = significant_timesteps[ts_idx]\n",
    "            marker_x_base_level = timestep\n",
    "            marker_y_base_level = non_norm_subj_df.reset_index()[feature_name][timestep]\n",
    "            next_ts_y_level = non_norm_subj_df.reset_index()[feature_name][timestep + 1]\n",
    "\n",
    "            if timestep == 0:\n",
    "                previous_ts_y_level = marker_y_base_level\n",
    "            else:\n",
    "                previous_ts_y_level = non_norm_subj_df.reset_index()[feature_name][timestep - 1]\n",
    "\n",
    "            feature_range = non_norm_subj_df.reset_index()[feature_name].max() - non_norm_subj_df.reset_index()[feature_name].min()\n",
    "\n",
    "            # plot marker at maximum angle to y slope\n",
    "            if (next_ts_y_level == previous_ts_y_level) or \\\n",
    "                ((next_ts_y_level - marker_y_base_level < feature_range / 20) and (marker_y_base_level > previous_ts_y_level))\\\n",
    "                    or ((feature_name == 'Temperature') and (timestep == 47)):\n",
    "                marker_y_level = marker_y_base_level + 0.02 * feature_range\n",
    "                marker_x_level = marker_x_base_level\n",
    "                marker = 'v'\n",
    "            elif (next_ts_y_level > previous_ts_y_level) and (marker_y_base_level - previous_ts_y_level < feature_range / 20):\n",
    "                marker_y_level = marker_y_base_level - 0.02 * feature_range\n",
    "                marker_x_level = marker_x_base_level\n",
    "                marker = '^'\n",
    "            elif (next_ts_y_level > previous_ts_y_level):\n",
    "                marker_y_level = marker_y_base_level\n",
    "                marker_x_level = marker_x_base_level - 0.5\n",
    "                marker = '>'\n",
    "            else :\n",
    "                marker_y_level = marker_y_base_level\n",
    "                marker_x_level = marker_x_base_level + 0.5\n",
    "                marker = '<'\n",
    "\n",
    "            twin_xs[-1].scatter(marker_x_level, marker_y_level, color=feature_color_dict[feature], s=100, marker=marker, alpha=1, edgecolors='white')\n",
    "\n",
    "\n",
    "axr.set_xlabel('Time from admission (hours)', fontsize=label_font_size)\n",
    "if plot_normalized_features:\n",
    "    axr.set_ylabel('Normalized feature value', fontsize=label_font_size)\n",
    "axr.tick_params(axis='both', labelsize=tick_label_size)\n",
    "\n",
    "if plot_legend:\n",
    "    if plot_normalized_features:\n",
    "        # add a legend entry for the normalized features\n",
    "        legend_markers.insert(0, plt.Line2D([0,0],[0,0], color='grey', alpha=0.5))\n",
    "        legend_labels.insert(0, 'All features (normalized)')\n",
    "\n",
    "    if display_inflection_markers:\n",
    "        # add a legend entry for the inflection markers\n",
    "        ts_marker_down = plt.scatter([], [], marker='v', color='grey', s=50, alpha=0.8)\n",
    "        ts_marker_up = plt.scatter([], [], marker='^', color='grey', s=50, alpha=0.8)\n",
    "        ts_marker_left = plt.scatter([], [], marker='<', color='grey', s=50, alpha=0.8)\n",
    "        ts_marker_right = plt.scatter([], [], marker='>', color='grey', s=50, alpha=0.8)\n",
    "        ts_label = 'Significant impact on inflection of prediction'\n",
    "        legend_markers.append(ts_marker_down)\n",
    "        legend_labels.append(ts_label)\n",
    "\n",
    "    twin_xs[-1].legend(legend_markers, legend_labels, fontsize=label_font_size, title_fontsize=label_font_size,\n",
    "              handler_map={tuple: HandlerTuple(ndivide=None), str: LegendTitle({'fontsize': label_font_size})},\n",
    "                       loc='lower right')\n",
    "\n",
    "# turn off grid\n",
    "axr.grid(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c380806",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_joint, (ax_main, ax_features) = plt.subplots(\n",
    "    nrows=2, \n",
    "    ncols=1, \n",
    "    figsize=(15, 12),\n",
    "    gridspec_kw=dict(height_ratios=[2, 1], hspace=0.3)\n",
    ")\n",
    "\n",
    "\n",
    "#### FIGURE 3 ####\n",
    "threshold = 0.04\n",
    "n_features_selection = 0\n",
    "n_features = 1\n",
    "k=0.25\n",
    "alpha=0.3\n",
    "only_non_static_features = True\n",
    "use_smoothed_shap_values = True\n",
    "plot_ground_truth = True\n",
    "display_significant_slopes = True\n",
    "n_slope_steps = 5\n",
    "slope_threshold = 1.5 * threshold\n",
    "\n",
    "skip_label_at_zero = True\n",
    "display_text_labels = True\n",
    "display_legend = True\n",
    "display_title = False\n",
    "plot_NIHSS_continuously = True\n",
    "ts_marker_level = 'shap' # 'baseline' (marker on predicted probability function) or 'shap' (marker on SHAP value function)\n",
    "\n",
    "tick_label_size = 13\n",
    "label_font_size = 16\n",
    "\n",
    "if use_smoothed_shap_values:\n",
    "    working_shap_values = smoothed_shap_values_over_time\n",
    "else:\n",
    "    working_shap_values = shap_values_over_time\n",
    "\n",
    "# identify significant changes in prediction over time by a change in threshold X% in prediction\n",
    "significant_positive_timesteps = filter_consecutive_numbers(np.where(np.diff(subj_pred_over_ts) > threshold)[0])\n",
    "significant_negative_timesteps = filter_consecutive_numbers(np.where(np.diff(subj_pred_over_ts) < -threshold)[0])\n",
    "significant_timesteps = np.concatenate((significant_positive_timesteps, significant_negative_timesteps))\n",
    "\n",
    "non_norm_subj_df = feature_values_df[feature_values_df.case_admission_id_idx == subj].drop(columns=['case_admission_id_idx']).pivot(index='time_step', columns='feature', values='feature_value')\n",
    "\n",
    "# for each timestep, identify the feature that has the largest impact on the prediction\n",
    "if only_non_static_features:\n",
    "    # find non static columns in non normed df\n",
    "    non_static_features = np.where(non_norm_subj_df.std() > 0.01)[0]\n",
    "    if use_simplified_shap_values:\n",
    "       non_static_features = np.where(np.isin(reduced_feature_names, np.array(non_norm_subj_df.std()[non_norm_subj_df.std() > 0.01].index)))[0]\n",
    "    selected_positive_features_by_impact = np.diff(working_shap_values[:, subj, non_static_features], axis=0)[significant_positive_timesteps].argmax(axis=1)\n",
    "    selected_positive_features_by_impact = non_static_features[selected_positive_features_by_impact]\n",
    "    selected_negative_features_by_impact = np.diff(working_shap_values[:, subj, non_static_features], axis=0)[significant_negative_timesteps].argmin(axis=1)\n",
    "    selected_negative_features_by_impact = non_static_features[selected_negative_features_by_impact]\n",
    "else:\n",
    "    selected_positive_features_by_impact = np.diff(working_shap_values[:, subj], axis=0)[significant_positive_timesteps].argmax(axis=1)\n",
    "    selected_negative_features_by_impact = np.diff(working_shap_values[:, subj], axis=0)[significant_negative_timesteps].argmin(axis=1)\n",
    "\n",
    "selected_features_by_impact = np.concatenate((selected_positive_features_by_impact, selected_negative_features_by_impact))\n",
    "\n",
    "if display_significant_slopes:\n",
    "    # identify features that are driving the change in prediction over time more gentle slopes (then filter out consecutive timesteps)\n",
    "    significant_positive_slope = filter_consecutive_numbers(set(np.where((np.concatenate((subj_pred_over_ts[n_slope_steps:], np.zeros(n_slope_steps))) - subj_pred_over_ts)[:-n_slope_steps] > slope_threshold)[0]).difference(set(significant_positive_timesteps)))\n",
    "\n",
    "    significant_negative_slope = filter_consecutive_numbers(set(np.where((np.concatenate((subj_pred_over_ts[n_slope_steps:], np.zeros(n_slope_steps))) - subj_pred_over_ts)[:-n_slope_steps] < -slope_threshold)[0]).difference(set(significant_negative_timesteps)))\n",
    "\n",
    "    delta_shap_by_features = np.concatenate((working_shap_values[n_slope_steps:, subj, non_static_features], np.zeros((n_slope_steps, len(non_static_features))))) - working_shap_values[:, subj, non_static_features]\n",
    "    selected_positive_features_by_slope = delta_shap_by_features[:-n_slope_steps][significant_positive_slope].argmax(axis=1)\n",
    "    selected_positive_features_by_slope = non_static_features[selected_positive_features_by_slope]\n",
    "    selected_negative_features_by_slope = delta_shap_by_features[:-n_slope_steps][significant_negative_slope].argmin(axis=1)\n",
    "    selected_negative_features_by_slope = non_static_features[selected_negative_features_by_slope]\n",
    "\n",
    "    selected_features_by_impact = np.concatenate((selected_features_by_impact, selected_positive_features_by_slope, selected_negative_features_by_slope))\n",
    "    significant_timesteps = np.concatenate((significant_timesteps, significant_positive_slope, significant_negative_slope))\n",
    "    selected_positive_features_by_impact = np.concatenate((selected_positive_features_by_impact, selected_positive_features_by_slope))\n",
    "    selected_negative_features_by_impact = np.concatenate((selected_negative_features_by_impact, selected_negative_features_by_slope))\n",
    "\n",
    "if n_features_selection == 0:\n",
    "    selected_positive_features = np.array([])\n",
    "    selected_negative_features = np.array([])\n",
    "else:\n",
    "    selected_positive_features = working_shap_values[-1,subj].argsort()[-n_features:][::-1]\n",
    "    selected_negative_features = working_shap_values[-1,subj].argsort()[:n_features][::-1]\n",
    "\n",
    "selected_features = np.concatenate((selected_positive_features, selected_positive_features_by_impact, selected_negative_features, selected_negative_features_by_impact)).astype(int)\n",
    "\n",
    "\n",
    "positive_color_palette = sns.color_palette(\"mako\", n_colors=len(set(np.concatenate((selected_positive_features, selected_positive_features_by_impact)))))\n",
    "negative_color_palette = sns.color_palette(\"flare_r\", n_colors=len(set(np.concatenate((selected_negative_features, selected_negative_features_by_impact)))))\n",
    "\n",
    "# plot prediction over time\n",
    "timestep_axis = np.array(range(n_time_steps))\n",
    "sns.lineplot(x=timestep_axis, y=subj_pred_over_ts, label='Predicted probability', linewidth = 2, ax=ax_main)\n",
    "\n",
    "# plot ground truth over time\n",
    "if plot_ground_truth:\n",
    "    # get changes in subj_gt_over_ts\n",
    "    changes_in_gt = np.diff(subj_gt_over_ts, prepend=0)\n",
    "    # pair np.where(changes_in_gt == -1) and np.where(changes_in_gt == 1\n",
    "    change_pairs = list(zip(np.where(changes_in_gt == 1)[0], np.where(changes_in_gt == -1)[0]))\n",
    "    for change_pair in change_pairs:\n",
    "        # mark a thick horizontal red line at the ground truth value (-1)\n",
    "        ax_main.plot([change_pair[0], change_pair[1]], [0, 0], color=\"#7b002c\", linewidth=10, alpha=0.8)\n",
    "        ax_main.text(np.mean(change_pair), 0 + 0.02, '6h to END', horizontalalignment='center', verticalalignment='center', fontsize=tick_label_size)\n",
    "\n",
    "\n",
    "pos_baseline = subj_pred_over_ts\n",
    "neg_baseline = subj_pred_over_ts\n",
    "pos_count = 0\n",
    "neg_count = 0\n",
    "feature_color_dict = {}\n",
    "for i, feature in enumerate(set(selected_features)):\n",
    "    subj_feature_shap_value_over_time = working_shap_values[:, subj, feature]\n",
    "    positive_portion = (subj_feature_shap_value_over_time > 0)\n",
    "    negative_portion = (subj_feature_shap_value_over_time < 0)\n",
    "\n",
    "\n",
    "    pos_function = subj_feature_shap_value_over_time.copy()\n",
    "    neg_function = subj_feature_shap_value_over_time.copy()\n",
    "    pos_function[negative_portion] = 0\n",
    "    neg_function[positive_portion] = 0\n",
    "\n",
    "    if feature in selected_features_by_impact:\n",
    "        important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "        # set value to zero before the significant timestep (except for NIHSS if plotting continuously)\n",
    "        if not np.logical_and(plot_NIHSS_continuously, reduced_feature_names[feature] == 'NIHSS'):\n",
    "            pos_function[:significant_timesteps[important_ts_idx][0] + 1] = 0\n",
    "            neg_function[:significant_timesteps[important_ts_idx][0] + 1] = 0\n",
    "\n",
    "    if feature in selected_positive_features:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    elif feature in selected_negative_features:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_negative_features_by_impact:\n",
    "        feature_color = negative_color_palette[neg_count]\n",
    "        neg_count += 1\n",
    "    elif feature in selected_positive_features_by_impact:\n",
    "        feature_color = positive_color_palette[pos_count]\n",
    "        pos_count += 1\n",
    "    feature_color_dict[feature] = feature_color\n",
    "\n",
    "    if np.any(pos_function):\n",
    "        positive_feature = pos_baseline + k * pos_function\n",
    "        ax_main.fill_between(timestep_axis , pos_baseline, positive_feature, color=feature_color, alpha=alpha)\n",
    "        pos_baseline = positive_feature\n",
    "\n",
    "    if np.any(neg_function):\n",
    "        negative_feature = neg_baseline + k * neg_function\n",
    "        ax_main.fill_between(timestep_axis, negative_feature, neg_baseline, color=feature_color, alpha=alpha)\n",
    "        neg_baseline = negative_feature\n",
    "\n",
    "    # add a legend entry for the feature fill\n",
    "    ax_main.scatter([], [], color=feature_color, alpha=alpha, label=reduced_feature_names[feature],marker=\"s\", s=200)\n",
    "\n",
    "\n",
    "# marking inflection points\n",
    "for feature in set(selected_features_by_impact):\n",
    "    important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "    for ts_idx in important_ts_idx:\n",
    "        if skip_label_at_zero and significant_timesteps[ts_idx] == 0:\n",
    "            continue\n",
    "        # downward inflection point\n",
    "        if subj_pred_over_ts[significant_timesteps[ts_idx]] > subj_pred_over_ts[significant_timesteps[ts_idx] + 1]:\n",
    "            marker = 'v'\n",
    "            if ts_marker_level == 'shap':\n",
    "                marker_y_level = pos_baseline[significant_timesteps[ts_idx]] + 0.005\n",
    "            elif ts_marker_level == 'baseline':\n",
    "                marker_y_level = subj_pred_over_ts[significant_timesteps[ts_idx]] + 0.005\n",
    "            text_y_level = marker_y_level + 0.01\n",
    "        # upward inflection point\n",
    "        else:\n",
    "            marker = '^'\n",
    "            if ts_marker_level == 'shap':\n",
    "                marker_y_level = neg_baseline[significant_timesteps[ts_idx]] - 0.005\n",
    "            elif ts_marker_level == 'baseline':\n",
    "                marker_y_level = subj_pred_over_ts[significant_timesteps[ts_idx]] - 0.005\n",
    "            text_y_level = marker_y_level - 0.015\n",
    "\n",
    "        ax_main.scatter(significant_timesteps[ts_idx], marker_y_level, color=feature_color_dict[feature], s=100, marker=marker, alpha=1, edgecolors='white')\n",
    "        # insert a label on the plot\n",
    "        if display_text_labels:\n",
    "            if marker == 'v':\n",
    "                ax_main.text(significant_timesteps[ts_idx]+ 0.01, text_y_level, reduced_feature_names[feature], fontsize=12, color='black',\n",
    "                                rotation=45, ha='left', va='bottom')\n",
    "            else:\n",
    "                ax_main.text(significant_timesteps[ts_idx]- 0.01, text_y_level, reduced_feature_names[feature], fontsize=12, color='black')\n",
    "\n",
    "\n",
    "if display_title:\n",
    "    ax_main.set_title(f'Predictions for subject {subj} of test set along time', fontsize=20)\n",
    "\n",
    "ax_main.set_xlabel('Time from admission (hours)', fontsize=label_font_size)\n",
    "ax_main.set_ylabel('Probability of END', fontsize=label_font_size)\n",
    "ax_main.tick_params(axis='both', labelsize=tick_label_size)\n",
    "\n",
    "if display_legend:\n",
    "    legend_markers, legend_labels = ax_main.get_legend_handles_labels()\n",
    "\n",
    "    # shap value shades\n",
    "    shap_shades_markers = legend_markers[1:]\n",
    "    shap_shades_labels = legend_labels[1:]\n",
    "    legend_markers = [legend_markers[0]]\n",
    "    legend_labels = [legend_labels[0]]\n",
    "\n",
    "    # add a legend entry for the timestep markers\n",
    "    ts_marker_down = plt.scatter([], [], marker='v', color='grey', s=50, alpha=0.8)\n",
    "    ts_marker_up = plt.scatter([], [], marker='^', color='grey', s=50, alpha=0.8)\n",
    "    ts_label = 'Positive / Negative impact on inflection of prediction'\n",
    "    legend_markers.append((ts_marker_up, ts_marker_down))\n",
    "    legend_labels.append(ts_label)\n",
    "\n",
    "    # Add a subtitle for shape value shades\n",
    "    legend_markers.append('')\n",
    "    legend_labels.append('')\n",
    "    legend_markers.append('Weight & direction of influence on model prediction')\n",
    "    legend_labels.append('')\n",
    "\n",
    "    legend_markers += shap_shades_markers\n",
    "    legend_labels += shap_shades_labels\n",
    "\n",
    "    ax_main.legend(legend_markers, legend_labels, fontsize=label_font_size, title='Influence on model prediction', title_fontsize=label_font_size,\n",
    "              handler_map={tuple: HandlerTuple(ndivide=None), str: LegendTitle({'fontsize': label_font_size})}, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "##### Figure 4 #####\n",
    "n_features_small = len(set(selected_features_by_impact))\n",
    "\n",
    "if n_features_small > 0:\n",
    "    # Create subplot grid within the bottom subplot\n",
    "    cols = min(4, n_features_small)  # Max 4 columns for better readability\n",
    "    rows = (n_features_small + cols - 1) // cols\n",
    "    \n",
    "    # Create nested subplots\n",
    "    gs_nested = ax_features.figure.add_gridspec(rows, cols, \n",
    "                                               left=ax_features.get_position().x0,\n",
    "                                               right=ax_features.get_position().x1,\n",
    "                                               bottom=ax_features.get_position().y0,\n",
    "                                               top=ax_features.get_position().y1,\n",
    "                                               hspace=0.4, wspace=0.3)\n",
    "    \n",
    "    # Remove the original ax_features\n",
    "    ax_features.remove()\n",
    "    \n",
    "    for idx, feature in enumerate(set(selected_features_by_impact)):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "        ax_small = fig_joint.add_subplot(gs_nested[row, col])\n",
    "        \n",
    "        feature_name = reduced_feature_names[feature]\n",
    "        feature_color = feature_color_dict[feature]\n",
    "        feature_data = non_norm_subj_df[feature_name]\n",
    "        \n",
    "        # Sparkline-style plot\n",
    "        ax_small.plot(timestep_axis, feature_data, color=feature_color, linewidth=2)\n",
    "        ax_small.fill_between(timestep_axis, feature_data, alpha=0.3, color=feature_color)\n",
    "        \n",
    "        # Add inflection points\n",
    "        important_ts_idx = np.where(selected_features_by_impact == feature)[0]\n",
    "        for ts_idx in important_ts_idx:\n",
    "            timestep = significant_timesteps[ts_idx]\n",
    "            ax_small.scatter(timestep, feature_data.iloc[timestep], color=feature_color, s=60, zorder=5, edgecolors='white', linewidth=1)\n",
    "        \n",
    "        # Styling\n",
    "        ax_small.set_title(feature_name, fontsize=tick_label_size, color=feature_color, weight='bold')\n",
    "        ax_small.set_xlim(0, n_time_steps)\n",
    "        ax_small.spines['top'].set_visible(False)\n",
    "        ax_small.spines['right'].set_visible(False)\n",
    "        \n",
    "        # Show min and max values on y-axis\n",
    "        y_min, y_max = feature_data.min(), feature_data.max()\n",
    "        if y_min == y_max:\n",
    "            y_ticks = [y_min]\n",
    "        else:\n",
    "            y_ticks = [y_min, y_max]\n",
    "        ax_small.set_yticks(y_ticks)\n",
    "        ax_small.tick_params(labelsize=tick_label_size-2)\n",
    "\n",
    "        ax_small.set_ylim(y_min - 0.2 * (y_max - y_min), y_max + 0.2 * (y_max - y_min))\n",
    "        \n",
    "        # Only show x-label on bottom row\n",
    "        if row == rows - 1:\n",
    "            ax_small.set_xlabel('Time (h)', fontsize=tick_label_size-1)\n",
    "        else:\n",
    "            ax_small.set_xticklabels([])\n",
    "\n",
    "else:\n",
    "    # If no features, show message\n",
    "    ax_features.text(0.5, 0.5, 'No significant feature changes detected', \n",
    "                    transform=ax_features.transAxes, ha='center', va='center', \n",
    "                    fontsize=label_font_size, style='italic')\n",
    "    ax_features.set_xlim(0, 1)\n",
    "    ax_features.set_ylim(0, 1)\n",
    "    ax_features.axis('off')\n",
    "\n",
    "# remove spines\n",
    "ax_main.spines['top'].set_visible(False)\n",
    "ax_main.spines['right'].set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_joint.savefig(f'/Users/jk1/temp/opsum_end/testing/subj_{subj}_inference_plot.png', bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b219c78",
   "metadata": {},
   "source": [
    "#### Find interesting subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 10 subj with top difference in max and min prediction\n",
    "top_10_diff_subj = np.argsort(np.abs(predictions_over_time.max(axis=1) - predictions_over_time.min(axis=1)))[-100:][::-1]\n",
    "# find intersection with subjs with non zero sum ground truth\n",
    "top_10_diff_subj_gt = np.intersect1d(top_10_diff_subj, np.where(gt_over_time.sum(axis=1) > 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9abc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_diff_subj_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_over_time[9], gt_over_time[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa14e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ground truth and prediction for subj\n",
    "for subj in top_10_diff_subj_gt:\n",
    "    subj_pred_over_ts = predictions_over_time[subj]\n",
    "    subj_gt_over_ts = gt_over_time[subj]\n",
    "\n",
    "    ax = sns.lineplot(x=timestep_axis, y=subj_pred_over_ts, label='Predicted probability', linewidth = 2)\n",
    "    ax = sns.lineplot(x=timestep_axis, y=subj_gt_over_ts, label='Ground truth', linewidth = 2, color='black', linestyle='--')\n",
    "    ax.set_title(f'Predictions for subject {subj} of test set along time', fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c6720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opsum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
