{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ab03d2f",
   "metadata": {},
   "source": [
    "For shap, features are aggregated as features, avg_features, min_features, max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1cfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from prediction.utils.visualisation_helper_functions import hex_to_rgb_color, create_palette\n",
    "from colormath.color_objects import LabColor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_path = '/Users/jk1/temp/opsum_end/training/hyperopt/xgb_gridsearch/xgb_gs_20250513_154517/checkpoints_short_opsum_xgb_20250518_001112_cv_1/shap_explanations_over_time/tree_explainer_shap_values_over_ts.pkl'\n",
    "test_data_path = '/Users/jk1/temp/opsum_end/preprocessing/gsu_Extraction_20220815_prepro_09052025_220520/early_neurological_deterioration_train_data_splits/test_data_early_neurological_deterioration_ts0.8_rs42_ns5.pth'\n",
    "cat_encoding_path = '/Users/jk1/temp/opsum_end/preprocessing/gsu_Extraction_20220815_prepro_09052025_220520/logs_09052025_220520/categorical_variable_encoding.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the shap values\n",
    "with open(os.path.join(shap_values_path), 'rb') as handle:\n",
    "    original_shap_values = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a185cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_last_timestep = False\n",
    "if only_last_timestep:\n",
    "    # use predictions from last timestep (as it also produces output for other timesteps)\n",
    "    shap_values = [original_shap_values[-1]]\n",
    "\n",
    "else:\n",
    "    shap_values = [np.array([original_shap_values[i] for i in range(len(original_shap_values))]).swapaxes(0, 1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6057ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test= ch.load(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e44b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_test[0, 0, :, 2]\n",
    "\n",
    "# features, avg_features, min_features, max_features\n",
    "# create feature names for avg_features, min_features, max_features\n",
    "avg_features = [f'avg_{i}' for i in features]\n",
    "min_features = [f'min_{i}' for i in features]\n",
    "max_features = [f'max_{i}' for i in features]\n",
    "# combine all feature names\n",
    "aggregated_feature_names = features.tolist() + avg_features + min_features + max_features + ['base_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_over_subj_shap_values = np.median(shap_values, axis=0)\n",
    "median_over_time_shap_values = np.median(shap_values, axis=1)\n",
    "max_over_time_shap_values = np.max(shap_values, axis=1)\n",
    "idx_max_over_time_shap_values = np.argmax(shap_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_over_subj_shap_values_df = pd.DataFrame(data=median_over_subj_shap_values, columns = np.array(aggregated_feature_names))\n",
    "median_over_time_shap_values_df = pd.DataFrame(data=median_over_time_shap_values, columns = np.array(aggregated_feature_names))\n",
    "max_over_time_shap_values_df = pd.DataFrame(data=max_over_time_shap_values, columns = np.array(aggregated_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ec69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_over_time_shap_values_df.reset_index(inplace=True)\n",
    "median_over_time_shap_values_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)\n",
    "median_over_time_shap_values_df = median_over_time_shap_values_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='shap_value')\n",
    "\n",
    "median_over_subj_shap_values_df.reset_index(inplace=True)\n",
    "median_over_subj_shap_values_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)\n",
    "median_over_subj_shap_values_df = median_over_subj_shap_values_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='shap_value')\n",
    "\n",
    "max_over_time_shap_values_df.reset_index(inplace=True)\n",
    "max_over_time_shap_values_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)\n",
    "max_over_time_shap_values_df = max_over_time_shap_values_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='shap_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d997f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_np = X_test[:, :, :, -1].astype('float32')\n",
    "avg_over_time_feature_values_df =  pd.DataFrame(data=test_X_np.mean(axis=(1)), columns = features)\n",
    "avg_over_time_feature_values_df = avg_over_time_feature_values_df.reset_index()\n",
    "avg_over_time_feature_values_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)\n",
    "avg_over_time_feature_values_df = avg_over_time_feature_values_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='feature_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_at_max_shap_list = []\n",
    "for subj_i in range(test_X_np.shape[0]):\n",
    "    feature_at_max_shap_subj_list = []\n",
    "    for feature_i in range(test_X_np.shape[2]):\n",
    "        feature_at_max_shap_subj_list.append(test_X_np[subj_i, idx_max_over_time_shap_values[subj_i, feature_i], feature_i])\n",
    "    feature_at_max_shap_list.append(np.array(feature_at_max_shap_subj_list))\n",
    "feature_at_max_shap = np.array(feature_at_max_shap_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0871df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_at_max_shap_df = pd.DataFrame(data=feature_at_max_shap, columns=features)\n",
    "feature_at_max_shap_df.reset_index(inplace=True)\n",
    "feature_at_max_shap_df.rename(columns={'index': 'case_admission_id_idx'}, inplace=True)\n",
    "feature_at_max_shap_df = feature_at_max_shap_df.melt(id_vars='case_admission_id_idx',  var_name='feature', value_name='feature_value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5823c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_over_time_features_with_shap_values_df = pd.merge(median_over_time_shap_values_df, avg_over_time_feature_values_df, on=['case_admission_id_idx', 'feature'], how='left')\n",
    "features_at_max_shap_values = pd.merge(max_over_time_shap_values_df, feature_at_max_shap_df, on=['case_admission_id_idx', 'feature'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_shap_values = features_at_max_shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f0edd",
   "metadata": {},
   "source": [
    "## Category preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096155ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_time_aggregated_features = True\n",
    "if pool_time_aggregated_features:\n",
    "    prefixes = ['avg_', 'min_', 'max_']\n",
    "    # remove starting prefixes from feature names\n",
    "    for prefix in prefixes:\n",
    "        features_with_shap_values['feature'] = features_with_shap_values['feature'].str.replace(f'^{prefix}', '', regex=True)\n",
    "    # sum the shap and feature values for each subject\n",
    "    features_with_shap_values = features_with_shap_values.groupby(['case_admission_id_idx', 'feature']).sum().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651dcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_categorical_encoding = True\n",
    "\n",
    "if reverse_categorical_encoding:\n",
    "    cat_encoding_df = pd.read_csv(cat_encoding_path)\n",
    "    for i in range(len(cat_encoding_df)):\n",
    "        cat_basename = cat_encoding_df.sample_label[i].lower().replace(' ', '_')\n",
    "        cat_item_list = cat_encoding_df.other_categories[i].replace('[', '').replace(']', '').replace('\\'', '').split(', ')\n",
    "        cat_item_list = [cat_basename + '_' + item.replace(' ', '_').lower() for item in cat_item_list]\n",
    "        for cat_item_idx, cat_item in enumerate(cat_item_list):\n",
    "            prefixes = ['', 'avg_', 'min_', 'max_']\n",
    "            for prefix in prefixes:\n",
    "                #  retrieve the dominant category for this subject (0 being default category)\n",
    "                cat_item_with_prefix = prefix + cat_item\n",
    "                features_with_shap_values.loc[features_with_shap_values.feature == cat_item_with_prefix, 'feature_value'] *= cat_item_idx + 1\n",
    "                features_with_shap_values.loc[features_with_shap_values.feature == cat_item_with_prefix, 'feature'] = prefix + cat_encoding_df.sample_label[i]\n",
    "                # sum the shap and feature values for each subject\n",
    "                features_with_shap_values = features_with_shap_values.groupby(['case_admission_id_idx', 'feature']).sum().reset_index()\n",
    "\n",
    "    # give a numerical encoding to the categorical features (manually extracted from the categorical_variable_encoding.csv file)\n",
    "    cat_to_numerical_encoding = {\n",
    "        'Prestroke disability (Rankin)': {0:0, 1:3, 2:4, 3:2, 4:1, 5:5},\n",
    "        'categorical_onset_to_admission_time': {0:2, 1:1, 2:0, 3:3, 4:5, 5:4},\n",
    "        'categorical_IVT': {0:2, 1:3, 2:4, 3:1, 4:0},\n",
    "        'categorical_IAT': {0:1, 1:2, 2:3, 3:0}\n",
    "    }\n",
    "\n",
    "    for cat_feature, cat_encoding in cat_to_numerical_encoding.items():\n",
    "        prefixes = ['', 'avg_', 'min_', 'max_']\n",
    "        for prefix in prefixes:\n",
    "            cat_feature = prefix + cat_feature\n",
    "            features_with_shap_values.loc[features_with_shap_values.feature == cat_feature, 'feature_value'] = features_with_shap_values.loc[features_with_shap_values.feature == cat_feature, 'feature_value'].map(cat_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c4abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_hourly_split_values = True\n",
    "\n",
    "# For features that are downsampled to hourly values, pool the values (median, min, max)\n",
    "\n",
    "if pool_hourly_split_values:\n",
    "    hourly_split_features = ['NIHSS', 'systolic_blood_pressure', 'diastolic_blood_pressure', 'mean_blood_pressure', 'heart_rate', 'respiratory_rate', 'temperature', 'oxygen_saturation']\n",
    "    prefixes = ['', 'avg_', 'min_', 'max_']\n",
    "    \n",
    "    for feature in hourly_split_features:\n",
    "        for prefix in prefixes:\n",
    "            features_with_shap_values.loc[features_with_shap_values.feature.str.contains(feature), 'feature'] = (prefix + feature[0].upper() + feature[1:]\n",
    ").replace('_', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd49d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace feature names with their english names\n",
    "feature_to_english_name_correspondence_path = os.path.join(os.path.dirname(os.path.abspath('__file__')),\n",
    "                                                           'preprocessing/preprocessing_tools/feature_name_to_english_name_correspondence.xlsx')\n",
    "feature_to_english_name_correspondence = pd.read_excel(feature_to_english_name_correspondence_path)\n",
    "\n",
    "for feature in features_with_shap_values.feature.unique():\n",
    "    if feature in feature_to_english_name_correspondence.feature_name.values:\n",
    "        features_with_shap_values.loc[features_with_shap_values.feature == feature, 'feature'] = feature_to_english_name_correspondence[feature_to_english_name_correspondence.feature_name == feature].english_name.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee316f72",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Select only the features that are in the top 10 most important features by mean absolute shap value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcbfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the top 10 most important features by mean absolute shap value\n",
    "features_with_shap_values['absolute_shap_value'] = np.abs(features_with_shap_values['shap_value'])\n",
    "top_10_features_by_mean_abs_summed_shap = features_with_shap_values.groupby('feature').mean().sort_values(by='absolute_shap_value', ascending=False).head(11).index.values\n",
    "# drop the 'base_value' feature from the top 10 features\n",
    "top_10_features_by_mean_abs_summed_shap = top_10_features_by_mean_abs_summed_shap[top_10_features_by_mean_abs_summed_shap != 'base_value']\n",
    "top_10_features_by_mean_abs_summed_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the top 10 most important features by mean minimum shap value\n",
    "top_10_features_by_mean_min_shap = features_with_shap_values.groupby('feature')['shap_value'].mean().nsmallest(10).index.values\n",
    "# drop the 'base_value' feature from the top 10 features\n",
    "top_10_features_by_mean_min_shap = top_10_features_by_mean_min_shap[top_10_features_by_mean_min_shap != 'base_value']\n",
    "\n",
    "# identify the top 10 most important features by mean maximum shap value\n",
    "top_10_features_by_mean_max_shap = features_with_shap_values.groupby('feature')['shap_value'].mean().nlargest(10).index.values\n",
    "# drop the 'base_value' feature from the top 10 features\n",
    "top_10_features_by_mean_max_shap = top_10_features_by_mean_max_shap[top_10_features_by_mean_max_shap != 'base_value']\n",
    "\n",
    "top_10_features_by_mean_min_shap, top_10_features_by_mean_max_shap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50886dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = top_10_features_by_mean_max_shap\n",
    "selected_features_with_shap_values_df = features_with_shap_values[features_with_shap_values.feature.isin(selected_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.legend_handler import HandlerTuple\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def plot_top_features_shap(selected_features_with_shap_values_df, selected_features,\n",
    "        ax,\n",
    "    plot_shap_direction_label = True,\n",
    "    plot_legend = True,\n",
    "    plot_colorbar = True,\n",
    "    plot_feature_value_along_y = False,\n",
    "    reverse_outcome_direction = False,   \n",
    "    tick_label_size = 11,\n",
    "    label_font_size = 13,\n",
    "    row_height = 0.4,\n",
    "    alpha = 0.8,\n",
    "    xlim:tuple = None\n",
    "    ):\n",
    "    \n",
    "    # Define the color palette\n",
    "    start_color = '#012D98'\n",
    "    end_color = '#f61067'\n",
    "    number_of_colors = 50\n",
    "    start_rgb = hex_to_rgb_color(start_color)\n",
    "    end_rgb = hex_to_rgb_color(end_color)\n",
    "    palette = create_palette(start_rgb, end_rgb, number_of_colors, LabColor, extrapolation_length=1)\n",
    "      \n",
    "    \n",
    "    for pos, feature in enumerate(selected_features[::-1]):\n",
    "        shaps = selected_features_with_shap_values_df[selected_features_with_shap_values_df.feature.isin([feature])].shap_value.values\n",
    "        values = selected_features_with_shap_values_df[selected_features_with_shap_values_df.feature.isin([feature])].feature_value\n",
    "        ax.axhline(y=pos, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
    "    \n",
    "        values = np.array(values, dtype=np.float64)  # make sure this can be numeric\n",
    "    \n",
    "        N = len(shaps)\n",
    "        nbins = 100\n",
    "        quant = np.round(nbins * (shaps - np.min(shaps)) / (np.max(shaps) - np.min(shaps) + 1e-8))\n",
    "        inds = np.argsort(quant + np.random.randn(N) * 1e-6)\n",
    "        layer = 0\n",
    "        last_bin = -1\n",
    "    \n",
    "        if plot_feature_value_along_y:\n",
    "            ys = values.copy()\n",
    "            cluster_factor = 0.1\n",
    "            for ind in inds:\n",
    "                if quant[ind] != last_bin:\n",
    "                    layer = 0\n",
    "                ys[ind] += cluster_factor * (np.ceil(layer / 2) * ((layer % 2) * 2 - 1))\n",
    "                layer += 1\n",
    "                last_bin = quant[ind]\n",
    "    \n",
    "        else:\n",
    "            ys = np.zeros(N)\n",
    "            cluster_factor = 1\n",
    "            for ind in inds:\n",
    "                if quant[ind] != last_bin:\n",
    "                    layer = 0\n",
    "                ys[ind] = cluster_factor * (np.ceil(layer / 2) * ((layer % 2) * 2 - 1))\n",
    "                layer += 1\n",
    "                last_bin = quant[ind]\n",
    "    \n",
    "        ys *= 0.9 * (row_height / np.max(ys + 1))\n",
    "    \n",
    "        # trim the color range, but prevent the color range from collapsing\n",
    "        vmin = np.nanpercentile(values, 5)\n",
    "        vmax = np.nanpercentile(values, 95)\n",
    "        if vmin == vmax:\n",
    "            vmin = np.nanpercentile(values, 1)\n",
    "            vmax = np.nanpercentile(values, 99)\n",
    "            if vmin == vmax:\n",
    "                vmin = np.min(values)\n",
    "                vmax = np.max(values)\n",
    "        if vmin > vmax: # fixes rare numerical precision issues\n",
    "            vmin = vmax\n",
    "    \n",
    "        # plot the non-nan values colored by the trimmed feature value\n",
    "        cvals = values.astype(np.float64)\n",
    "        cvals_imp = cvals.copy()\n",
    "        cvals_imp[np.isnan(cvals)] = (vmin + vmax) / 2.0\n",
    "        cvals[cvals_imp > vmax] = vmax\n",
    "        cvals[cvals_imp < vmin] = vmin\n",
    "        ax.scatter(shaps, pos + ys,\n",
    "                   cmap=ListedColormap(palette), vmin=vmin, vmax=vmax, s=16,\n",
    "                   c=cvals, alpha=alpha, linewidth=0,\n",
    "                   zorder=3, rasterized=len(shaps) > 500)\n",
    "    \n",
    "    \n",
    "    \n",
    "    axis_color=\"#333333\"\n",
    "    if plot_colorbar:\n",
    "        m = cm.ScalarMappable(cmap=ListedColormap(palette))\n",
    "        m.set_array([0, 1])\n",
    "        \n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes('right', size='5%', pad=0.5)\n",
    "\n",
    "        # get fig from ax\n",
    "        fig = ax.get_figure()\n",
    "        cb = fig.colorbar(m, ticks=[0, 1], aspect=10, shrink=0.2, ax=cax)\n",
    "        cb.set_ticklabels(['Low', 'High'])\n",
    "        cb.ax.tick_params(labelsize=tick_label_size, length=0)\n",
    "        cb.set_label('Feature value', size=label_font_size, backgroundcolor=\"white\")\n",
    "        cb.ax.yaxis.set_label_position('left')\n",
    "        cb.set_alpha(1)\n",
    "        cb.outline.set_visible(False)\n",
    "        # turn off grid and spines on cax\n",
    "        cax.grid(False)\n",
    "        cax.spines['right'].set_visible(False)\n",
    "        cax.spines['top'].set_visible(False)\n",
    "        cax.spines['left'].set_visible(False)\n",
    "        cax.spines['bottom'].set_visible(False)\n",
    "        cax.set_xticks([])\n",
    "        cax.set_yticks([])\n",
    "\n",
    "    \n",
    "    if plot_legend:\n",
    "        legend_markers = []\n",
    "        legend_labels = []\n",
    "        single_dot = mlines.Line2D([], [], color=palette[len(palette)//2], marker='.', linestyle='None',\n",
    "                              markersize=10)\n",
    "        single_dot_label = 'Single Patient\\n(summed over time)'\n",
    "        legend_markers.append(single_dot)\n",
    "        legend_labels.append(single_dot_label)\n",
    "    \n",
    "        ax.legend(legend_markers, legend_labels, title='SHAP/Feature values', fontsize=tick_label_size, title_fontsize=label_font_size,\n",
    "                  handler_map={tuple: HandlerTuple(ndivide=None)},\n",
    "                         loc='lower right', frameon=True)\n",
    "    \n",
    "    \n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.tick_params(color=axis_color, labelcolor=axis_color)\n",
    "    \n",
    "    yticklabels = selected_features[::-1]\n",
    "    ax.set_yticks(range(len(selected_features_with_shap_values_df.feature.unique())))\n",
    "    ax.set_yticklabels(yticklabels, fontsize=label_font_size)\n",
    "    ax.tick_params('y', length=20, width=0.5, which='major')\n",
    "    ax.tick_params('x', labelsize=tick_label_size)\n",
    "    ax.set_ylim(-1, len(selected_features_with_shap_values_df.feature.unique()))\n",
    "    ax.set_xlabel('SHAP Value \\n(impact on model output)', fontsize=label_font_size)\n",
    "    ax.grid(color='white', axis='y')\n",
    "    \n",
    "    if xlim:\n",
    "        ax.set_xlim(xlim[0], xlim[1])\n",
    "    \n",
    "    # Plot additional explanation with the shap value X axis\n",
    "    if plot_shap_direction_label:\n",
    "        x_ticks_coordinates = ax.get_xticks()\n",
    "        x_ticks_labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "        # let x tick label be the coordinate with 2 decimals\n",
    "    \n",
    "        if reverse_outcome_direction:\n",
    "            x_ticks_labels = [f'{x_ticks_coordinate:.1f}' for x_ticks_coordinate in x_ticks_coordinates]\n",
    "            x_ticks_labels[0] = f'Toward better\\noutcome'\n",
    "            x_ticks_labels[-1] = f'Toward worse\\noutcome'\n",
    "        else:\n",
    "            x_ticks_labels = [f'{x_ticks_coordinate:.1f}' for x_ticks_coordinate in x_ticks_coordinates]\n",
    "            x_ticks_labels[0] = f'Toward worse\\noutcome'\n",
    "            x_ticks_labels[-1] = f'Toward better\\noutcome'\n",
    "    \n",
    "        ax.set_xticks(x_ticks_coordinates)\n",
    "        ax.set_xticklabels(x_ticks_labels)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39803c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "reverse_outcome_direction = True\n",
    "plot_top_features_shap(selected_features_with_shap_values_df, selected_features,\n",
    "    ax,\n",
    "    reverse_outcome_direction=reverse_outcome_direction,\n",
    "    xlim=(-0.6, 0.6)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig.savefig('/Users/jk1/temp/opsum_end/testing/features_at_max_shap_value.png', bbox_inches='tight', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b84cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opsum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
