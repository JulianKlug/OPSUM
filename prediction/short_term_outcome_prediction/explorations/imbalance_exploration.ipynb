{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2901e3aa12bb891c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "weight adjustment for imbalanced dataset \n",
    "\n",
    "methods:  \n",
    "- BCEWithLogitsLoss https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "- oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d26a9baf9cf1a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T13:58:05.530888Z",
     "start_time": "2025-03-05T13:58:01.693207Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch as ch\n",
    "import pandas as pd\n",
    "import os\n",
    "from prediction.short_term_outcome_prediction.timeseries_decomposition import decompose_and_label_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T13:58:05.637654Z",
     "start_time": "2025-03-05T13:58:05.633145Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = '/Users/jk1/temp/opsum_end/preprocessing/gsu_Extraction_20220815_prepro_08062024_083500/early_neurological_deterioration_train_data_splits/train_data_splits_early_neurological_deterioration_ts0.8_rs42_ns5.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e403a71f3cd5f85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T13:58:06.407297Z",
     "start_time": "2025-03-05T13:58:06.305893Z"
    }
   },
   "outputs": [],
   "source": [
    "use_gpu = ch.cuda.is_available()\n",
    "target_time_to_outcome = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfa857998d554d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T13:58:06.806794Z",
     "start_time": "2025-03-05T13:58:06.489924Z"
    }
   },
   "outputs": [],
   "source": [
    "splits = ch.load(os.path.join(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bb896a5e1e071",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T14:14:37.670901Z",
     "start_time": "2024-06-30T14:14:27.776214Z"
    }
   },
   "outputs": [],
   "source": [
    "# recommended weight adjustement\n",
    "recommended_weight_adjustment_df = pd.DataFrame(columns=['split', 'train', 'val'])\n",
    "for idx, split in enumerate(splits):\n",
    "    X_train, X_val, y_train, y_val = split\n",
    "    train_map, train_flat_labels = decompose_and_label_timeseries(X_train, y_train, target_time_to_outcome=target_time_to_outcome)\n",
    "    val_map, val_flat_labels = decompose_and_label_timeseries(X_val, y_val, target_time_to_outcome=target_time_to_outcome)\n",
    "    \n",
    "    n_pos_train = sum(train_flat_labels) + 1e-6\n",
    "    n_neg_train = len(train_flat_labels) - n_pos_train\n",
    "    n_pos_val = sum(val_flat_labels) + 1e-6\n",
    "    n_neg_val = len(val_flat_labels) - n_pos_val\n",
    "\n",
    "    print(f'For split {idx}:')\n",
    "    print(f'Number of positive samples in train: {n_pos_train} ({n_pos_train/len(train_flat_labels):.2%})')\n",
    "    print(f'Number of negative samples in train: {n_neg_train} ({n_neg_train/len(train_flat_labels):.2%})')\n",
    "    print(f'Number of positive samples in val: {n_pos_val} ({n_pos_val/len(val_flat_labels):.2%})')\n",
    "    print(f'Number of negative samples in val: {n_neg_val} ({n_neg_val/len(val_flat_labels):.2%})')\n",
    "\n",
    "    print(f'Weight adjustment for train: {n_neg_train/n_pos_train}')\n",
    "    print(f'Weight adjustment for val: {n_neg_val/n_pos_val}')\n",
    "    \n",
    "    recommended_weight_adjustment_df = pd.concat([recommended_weight_adjustment_df, pd.DataFrame([[idx, n_neg_train/n_pos_train, n_neg_val/n_pos_val]], columns=['split', 'train', 'val'])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122e07e27cb0329b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T14:14:37.692401Z",
     "start_time": "2024-06-30T14:14:37.675701Z"
    }
   },
   "outputs": [],
   "source": [
    "recommended_weight_adjustment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47688d0a2b59c418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-30T14:14:37.700370Z",
     "start_time": "2024-06-30T14:14:37.694361Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_average = recommended_weight_adjustment_df[['train', 'val']].median()\n",
    "overall_average.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435e7c3",
   "metadata": {},
   "source": [
    "Test BucketBatchSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb897cf9bd0e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.short_term_outcome_prediction.timeseries_decomposition import prepare_subsequence_dataset\n",
    "\n",
    "\n",
    "all_datasets = [prepare_subsequence_dataset(x, use_gpu=use_gpu) for x in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = all_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b151a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction.short_term_outcome_prediction.timeseries_decomposition import BucketBatchSampler\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "\n",
    "train_bucket_sampler = BucketBatchSampler(train_dataset.idx_to_len_map, batch_size,\n",
    "                                              labels=train_dataset.targets,  # Pass the target labels\n",
    "    oversampling_factor=1\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_bucket_sampler,\n",
    "                                  # shuffling is done in the bucket sampler\n",
    "                                  shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c576ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through train_loader and get number of positive and negative samples on each batch\n",
    "n_total_pos = 0\n",
    "n_total_neg = 0\n",
    "\n",
    "for batch in train_loader:\n",
    "    # get the labels\n",
    "    labels = batch[1]\n",
    "    # get the number of positive and negative samples\n",
    "    n_pos = sum(labels)\n",
    "    n_neg = len(labels) - n_pos\n",
    "    n_total_pos += n_pos\n",
    "    n_total_neg += n_neg\n",
    "    print(f'Number of positive samples in batch: {n_pos} ({n_pos/len(labels):.2%})')\n",
    "    print(f'Number of negative samples in batch: {n_neg} ({n_neg/len(labels):.2%})')\n",
    "\n",
    "print(f'Number of positive samples in train: {n_total_pos} ({n_total_pos/(n_total_pos+n_total_neg):.2%})')\n",
    "print(f'Number of negative samples in train: {n_total_neg} ({n_total_neg/(n_total_pos+n_total_neg):.2%})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0219d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "val_bucket_sampler = BucketBatchSampler(val_dataset.idx_to_len_map, batch_size,\n",
    "                                              labels=val_dataset.targets,  # Pass the target labels\n",
    "    oversampling_factor=1\n",
    ")\n",
    "val_loader = DataLoader(val_dataset, batch_sampler=val_bucket_sampler,\n",
    "                                  # shuffling is done in the bucket sampler\n",
    "                                  shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380dc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_dataset.targets.sum(), train_dataset.targets.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1fed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through val_loader and get number of positive and negative samples on each batch\n",
    "n_total_pos = 0\n",
    "n_total_neg = 0\n",
    "for batch in val_loader:\n",
    "    # get the labels\n",
    "    labels = batch[1]\n",
    "    # get the number of positive and negative samples\n",
    "    n_pos = sum(labels)\n",
    "    n_neg = len(labels) - n_pos\n",
    "    n_total_pos += n_pos\n",
    "    n_total_neg += n_neg\n",
    "    print(f'Number of positive samples in batch: {n_pos} ({n_pos/len(labels):.2%})')\n",
    "    print(f'Number of negative samples in batch: {n_neg} ({n_neg/len(labels):.2%})')\n",
    "print(f'Number of positive samples in val: {n_total_pos} ({n_total_pos/(n_total_pos+n_total_neg):.2%})')\n",
    "print(f'Number of negative samples in val: {n_total_neg} ({n_total_neg/(n_total_pos+n_total_neg):.2%})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9923e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opsum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
